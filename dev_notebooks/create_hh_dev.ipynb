{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2407e9-b944-4b91-adac-ab121fcd9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process SASSIE ocean model granules on s3\n",
    "\n",
    "## import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import netCDF4 as nc4\n",
    "import tarfile\n",
    "import json\n",
    "import uuid as uuid\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import s3fs\n",
    "import argparse\n",
    "from pprint import pprint\n",
    "import time \n",
    "import gzip\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "import MITgcmutils\n",
    "\n",
    "## import ECCO utils\n",
    "import sys\n",
    "sys.path.append('/Users/mzahn/github_others/ECCOv4-py')\n",
    "import ecco_v4_py as ecco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f01aecb0-10a8-483d-bf72-0bfe520cd237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "def time_it(func):\n",
    "    \"\"\"\n",
    "    Decorator that reports the execution time.\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()  # Capture the start time\n",
    "        result = func(*args, **kwargs)  # Execute the function\n",
    "        end_time = time.time()  # Capture the end time\n",
    "        print(f\"{func.__name__} took {end_time-start_time:.4f} seconds to execute\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def is_valid_gzip_file(filepath):\n",
    "    try:\n",
    "        # https://stackoverflow.com/questions/33938173/verifying-file-integrity-with-python/33938986#33938986\n",
    "        # Attempt to open the gzip file in read mode\n",
    "        with gzip.open(filepath, 'rb') as f:\n",
    "             print(f'... opening {filepath}')\n",
    "             # if the gz is bad then seek to the end will fail\n",
    "             f.seek(-1, os.SEEK_END)\n",
    "             return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: caught for {filepath} {e}')\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def load_sassie_N1_field(file_dir, fname, nk=1, skip=0):\n",
    "    num_cols = 680*4 + 1080\n",
    "    num_rows = 1080\n",
    "    \n",
    "    time_level = int(fname.split('.data')[0].split('.')[-1])\n",
    "    \n",
    "    tmp_compact = ecco.load_binary_array(file_dir, fname, \\\n",
    "                                    num_rows, num_cols, nk=nk, skip=skip, \n",
    "                                    filetype='>f4', less_output=True)\n",
    "\n",
    "    return tmp_compact, time_level\n",
    "\n",
    "\n",
    "def sassie_n1_compact_to_faces_2D(sassie_n1_compact):\n",
    "    sassie_faces = dict()\n",
    "    n = 680\n",
    "    \n",
    "    # Face 1 \n",
    "    start_row = 0\n",
    "    end_row = n\n",
    "    sassie_faces[1] = sassie_n1_compact[start_row:end_row,:]\n",
    "\n",
    "    # Face 2\n",
    "    start_row = end_row\n",
    "    end_row = start_row + n\n",
    "\n",
    "    sassie_faces[2] = sassie_n1_compact[start_row:end_row,:]\n",
    "    \n",
    "    # Face 3\n",
    "    start_row = end_row\n",
    "    end_row = start_row + 1080\n",
    "    sassie_faces[3] = sassie_n1_compact[start_row:end_row:,:]\n",
    "    \n",
    "    #Face 4\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[4] = sassie_n1_compact[start_row:end_row].reshape(1080, n)\n",
    "\n",
    "    #Face 5\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[5] = sassie_n1_compact[start_row:end_row].reshape(1080, n)\n",
    "\n",
    "    return sassie_faces\n",
    "\n",
    "\n",
    "def sassie_n1_compact_to_faces_3D(sassie_n1_compact):\n",
    "    sassie_faces = dict()\n",
    "    n = 680\n",
    "    \n",
    "    # Face 1 \n",
    "    start_row = 0\n",
    "    end_row = n\n",
    "    sassie_faces[1] = sassie_n1_compact[:,start_row:end_row,:]\n",
    "\n",
    "    # Face 2\n",
    "    start_row = end_row\n",
    "    end_row = start_row + n\n",
    "    sassie_faces[2] = sassie_n1_compact[:,start_row:end_row,:]\n",
    "    \n",
    "    # Face 3\n",
    "    start_row = end_row\n",
    "    end_row = start_row + 1080\n",
    "    sassie_faces[3] = sassie_n1_compact[:,start_row:end_row:,:]\n",
    "    \n",
    "    #Face 4\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[4] = sassie_n1_compact[:,start_row:end_row].reshape(90, 1080, n)\n",
    "\n",
    "    #Face 5\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[5] = sassie_n1_compact[:,start_row:end_row].reshape(90, 1080, n)\n",
    "\n",
    "    return sassie_faces\n",
    "\n",
    "\n",
    "def combine_sassie_N1_faces_to_HHv2_2D(face_arr):\n",
    "    \"\"\"\n",
    "    2D function for scalar fields, c point\n",
    "    \"\"\"\n",
    "    # dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((1080, 1800)) \n",
    "    \n",
    "    # cut out sections we want and assign them to location on HH\n",
    "    new_arr[:, 185:185 + 1080] = face_arr[3]\n",
    "    \n",
    "    # rotate Face 1 to line up with orientation of Face 3\n",
    "    new_arr[:, 0:185] = np.flipud(face_arr[1][-185:,:].T) # flip and transpose\n",
    "    \n",
    "    new_arr[:, 185 + 1080:] = face_arr[4][:,:535]\n",
    "\n",
    "    new_arr = np.rot90(new_arr,2) # rotate it 180 so Greenland/AK are on bottom\n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def combine_sassie_N1_faces_to_HHv2_2D_u_point(face_arr_u, face_arr_v):\n",
    "    \"\"\"\n",
    "    2D function for vector fields, u point\n",
    "    \"\"\"\n",
    "    ## dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((1080, 1800))\n",
    "    \n",
    "    ## add Arctic face (3)\n",
    "    new_arr[:, 185:185+1080] = face_arr_u[3] # take entire Artic face\n",
    "    \n",
    "    ## add face 1 that will be flipped (must use v array)\n",
    "    new_arr[:, 0:185] = np.flipud(face_arr_v[1][-185:,:].T)\n",
    "        \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:, 185+1080:] = face_arr_u[4][:,:535]\n",
    "\n",
    "    ## rotate by 90 deg twice to have Alaska on bottom left\n",
    "    ## since it is vector field, have to multiply whole array by -1\n",
    "    new_arr = np.rot90(new_arr,2)\n",
    "    new_arr = new_arr *-1\n",
    "        \n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def combine_sassie_N1_faces_to_HHv2_2D_v_point(face_arr_v, face_arr_u, vec=False):\n",
    "    \"\"\"\n",
    "    2D function for vector fields, v point\n",
    "    \"\"\"\n",
    "    ## dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((1080, 1800))\n",
    "    \n",
    "    ## add Arctic face (3)\n",
    "    new_arr[:, 185:185+1080] = face_arr_v[3]\n",
    "    \n",
    "    ## add part of face 1 (Europe) that will be flipped (must use u array and multiply by -1)\n",
    "    \n",
    "    ## after rotating face 1, the u points on face 1 will not match the v points of face 3 (offset by 1 upwards)\n",
    "    ## therefore, must remove the first column of face 1 and add the first column from face 2 to the end of face 1\n",
    "    ## remove the first i column from the u field so the shape is (680, 1079) = (j,i)\n",
    "    face1_tmp = face_arr_u[1][:,1:]\n",
    "    ## then add the first row from face 2 to the end of face 1\n",
    "    face1_mod = np.concatenate((face1_tmp, face_arr_u[2][:,:1]), axis=1)\n",
    "    \n",
    "    ## add modified face 1 by rotating and multiplying by -1\n",
    "    new_arr[:, 0:185] = np.flipud(face1_mod[-185:,:].T)*-1\n",
    "    \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:, 185+1080:] = face_arr_v[4][:,:535]\n",
    "\n",
    "    ## rotate by 90 deg twice to have Alaska on bottom left\n",
    "    new_arr = np.rot90(new_arr,2)\n",
    "    new_arr = new_arr *-1\n",
    "        \n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def combine_sassie_N1_faces_to_HHv2_3D(face_arr):\n",
    "    \"\"\"\n",
    "    3D function for scalar fields, c point\n",
    "    \"\"\"\n",
    "    # dimensions of the final Arctic HH field. 535+185+1080=1800 ; 90 vertical levels\n",
    "    new_arr = np.zeros((90, 1080, 1800)) \n",
    "    \n",
    "    # cut out sections we want and assign them to location on HH\n",
    "    new_arr[:,:,185:185+1080] = face_arr[3]\n",
    "    \n",
    "    # rotate Face 1 to line up with orientation of Face 3\n",
    "    new_arr[:,:,0:185] = np.transpose(face_arr[1][:,-185:,::-1],axes=(0,2,1)) # flip and transpose\n",
    "    \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:,:,185+1080:] = face_arr[4][:,:,:535]\n",
    "    \n",
    "    ## rotate it 180 so Greenland/AK are on bottom\n",
    "    new_arr = np.rot90(new_arr,2,axes=(1,2)) \n",
    "    \n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def combine_sassie_N1_faces_to_HHv2_3D_u_point(face_arr_u, face_arr_v):\n",
    "    \"\"\"\n",
    "    3D function for vector fields, u point\n",
    "    \"\"\"\n",
    "    ## dimensions of the final Arctic HH field. 535+185+1080=1800 ; 90 vertical levels\n",
    "    new_arr = np.zeros((90, 1080, 1800))\n",
    "    \n",
    "    ## add Arctic face (3)\n",
    "    new_arr[:,:,185:185+1080] = face_arr_u[3] # take entire Artic face\n",
    "    \n",
    "    ## add face 1 that will be flipped (must use v array)\n",
    "    new_arr[:,:,0:185] = np.transpose(face_arr_v[1][:,-185:,::-1],axes=(0,2,1))\n",
    "        \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:,:,185+1080:] = face_arr_u[4][:,:,:535]\n",
    "\n",
    "    ## rotate by 90 deg twice to have Alaska on bottom left\n",
    "    ## since it is vector field, have to multiply whole array by -1\n",
    "    new_arr = np.rot90(new_arr,2,axes=(1,2))\n",
    "    new_arr = new_arr *-1\n",
    "        \n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def combine_sassie_N1_faces_to_HHv2_3D_v_point(face_arr_v, face_arr_u):\n",
    "    \"\"\"\n",
    "    3D function for vector fields, v point\n",
    "    \"\"\"\n",
    "    ## dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((90, 1080, 1800))\n",
    "    \n",
    "    ## add Arctic face (3)\n",
    "    new_arr[:,:,185:185+1080] = face_arr_v[3]\n",
    "    \n",
    "    ## add part of face 1 (Europe) that will be flipped (must use u array and multiply by -1)\n",
    "    \n",
    "    ## after rotating face 1, the u points on face 1 will not match the v points of face 3 (offset by 1 upwards)\n",
    "    ## therefore, must remove the first column of face 1 and add the first column from face 2 to the end of face 1\n",
    "    ## remove the first i column from the u field so the shape is (680, 1079) = (j,i)\n",
    "    face1_tmp = face_arr_u[1][:,:,1:]\n",
    "    ## then add the first row from face 2 to the end of face 1\n",
    "    face1_mod = np.concatenate((face1_tmp, face_arr_u[2][:,:,:1]), axis=2)\n",
    "    \n",
    "    ## add modified face 1 by rotating and multiplying by -1\n",
    "    new_arr[:,:,0:185] = np.transpose(face1_mod[:,-185:,::-1],axes=(0,2,1))*-1\n",
    "    \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:,:,185+1080:] = face_arr_v[4][:,:,:535]\n",
    "\n",
    "    ## rotate by 90 deg twice to have Alaska on bottom left\n",
    "    new_arr = np.rot90(new_arr,2,axes=(1,2))\n",
    "    new_arr = new_arr *-1\n",
    "        \n",
    "    return new_arr\n",
    "\n",
    "\n",
    "def timestamp_from_iter_num(iter_num):\n",
    "    \"\"\"\n",
    "    takes the model iteration that was pulled from the data's filename and converts it to its equivalent datetime\n",
    "    \"\"\"\n",
    "    ## Start time of the model is 5790000 (22.0319 years after 1992-01-01)\n",
    "    ## there are 120 seconds for each iteration and 86400 seconds per day\n",
    "    ## take the iteration number, convert to seconds, and calculate number of days since start of model\n",
    "    \n",
    "    ## from Mike: \"Near the end of the simulation, I ran into some sort of instability so I changed the time step from 120 seconds to 60 seconds.\n",
    "    ## Usually I would change it back to 120 second after getting past the instability but I was kinda close to the end so I just let it ride with 60 seconds.\"\n",
    "    if iter_num > 1e7:\n",
    "        iter_num = iter_num/2\n",
    "    \n",
    "    num_days_since_start = iter_num*120 / 86400 ## divide iter_number by 86400 which is equal to the number of seconds in a day\n",
    "    \n",
    "    model_start_time = datetime(1992,1,1) # data.cal start time is 1992-01-01\n",
    "    timestamp = np.array([model_start_time + timedelta(days=num_days_since_start)], dtype='datetime64[ns]')\n",
    "    \n",
    "    return timestamp\n",
    "\n",
    "def unpack_tar_gz_files(data_dir, keep_local_files):\n",
    "    ## see if tar.gz files were already decompressed\n",
    "    print(f'... looking for *.data files in {data_dir}')\n",
    "    data_files = list(data_dir.glob('*.data'))\n",
    "\n",
    "    if len(data_files)>0:\n",
    "        print(\"... tar.gz files already unpacked\")\n",
    "\n",
    "    ## if not, open them\n",
    "    else:\n",
    "        ## pull list of all tar.gz files in directory\n",
    "        tar_gz_files = list(data_dir.glob('*.tar.gz'))\n",
    "        \n",
    "        ## unzip targz file\n",
    "        for file_path in tar_gz_files:\n",
    "            print(f'... testing validity of {file_path}')\n",
    "\n",
    "            if is_valid_gzip_file(file_path):\n",
    "              try:\n",
    "                 tar = tarfile.open(file_path, \"r:gz\")\n",
    "                 tar.extractall(data_dir) # save files to same directory\n",
    "                 tar.close()\n",
    "              except Exception as e:\n",
    "                 print(f'ERROR: could not extractall {file_path}')\n",
    "\n",
    "            else:\n",
    "              print('ERROR: invalid gz file could not open {file_path}')\n",
    "             \n",
    "\n",
    "\n",
    "def show_me_the_ds(ds):\n",
    "    print('\\n>>show_me_the_ds')\n",
    "    print('dims: ', list(ds.dims))\n",
    "    print('coords: ', list(ds.coords))\n",
    "    print('data_vars: ', list(ds.data_vars))\n",
    "\n",
    "def show_me_the_da(da):\n",
    "    print('\\n>>show_me_the_da')\n",
    "    print('dims: ', list(da.dims))\n",
    "    print('coords: ',list(da.coords))\n",
    "    \n",
    "def make_2D_HHv2_ds(field_HH, model_grid_ds, timestamp, grid_point, da_name):\n",
    "    \n",
    "    ## get time bounds and center time\n",
    "    time_bnds, center_time = ecco.make_time_bounds_from_ds64(timestamp[0], 'AVG_DAY')\n",
    "    time_bnds_da = xr.DataArray(time_bnds.reshape(1,2), dims=['time', 'nv'])\n",
    "    \n",
    "    ## create DataArray for c point data\n",
    "    if grid_point == 'c':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','j','i'],\\\n",
    "                                coords={'time':(('time'), np.array([center_time]))})\n",
    "    \n",
    "    ## create DataArray for u point data\n",
    "    elif grid_point == 'u':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','j','i_g'],\\\n",
    "                                coords={'time':(('time'), np.array([center_time]))})\n",
    "    \n",
    "    ## create DataArray for v point data\n",
    "    elif grid_point == 'v':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','j_g','i'],\\\n",
    "                                coords={'time':(('time'), np.array([center_time]))})\n",
    "    \n",
    "    ## name the array\n",
    "    tmp_da.name = da_name\n",
    "    \n",
    "    ## add additional coordinates to dataset\n",
    "    tmp_ds = tmp_da.to_dataset().assign_coords({\n",
    "        'time_bnds':time_bnds_da,\\\n",
    "        'XC':model_grid_ds.XC,\\\n",
    "        'YC':model_grid_ds.YC,\\\n",
    "        'XG':model_grid_ds.XG,\\\n",
    "        'YG':model_grid_ds.YG,\\\n",
    "        'XC_bnds':model_grid_ds.XC_bnds,\\\n",
    "        'YC_bnds':model_grid_ds.YC_bnds})\n",
    "    \n",
    "    return tmp_ds\n",
    "\n",
    "\n",
    "def make_3D_HHv2_ds(field_HH, model_grid_ds, timestamp, grid_point, da_name, k_face='center'):\n",
    "    \n",
    "    ## get time bounds and center time\n",
    "    time_bnds, center_time = ecco.make_time_bounds_from_ds64(timestamp[0], 'AVG_DAY')\n",
    "    time_bnds_da = xr.DataArray(time_bnds.reshape(1,2), dims=['time', 'nv'])\n",
    "    \n",
    "    if k_face == 'center':\n",
    "        \n",
    "        ## create DataArray for c point data, center\n",
    "        if grid_point == 'c':\n",
    "            tmp_da = xr.DataArray([field_HH], dims=['time', 'k','j','i'],\\\n",
    "                                    coords={'time':(('time'),np.array([center_time]))})\n",
    "            tmp_da['k'].attrs['axis']  = 'Z'\n",
    "            \n",
    "        ## create DataArray for u point data, center\n",
    "        elif grid_point == 'u':\n",
    "            tmp_da = xr.DataArray([field_HH], dims=['time', 'k','j','i_g'],\\\n",
    "                                    coords={'time':(('time'),np.array([center_time]))})\n",
    "            tmp_da['k'].attrs['axis']  = 'Z'\n",
    "            \n",
    "        ## create DataArray for v point data, center\n",
    "        elif grid_point == 'v':\n",
    "            tmp_da = xr.DataArray([field_HH], dims=['time', 'k','j_g','i'],\\\n",
    "                                    coords={'time':(('time'),np.array([center_time]))})\n",
    "            tmp_da['k'].attrs['axis']  = 'Z'            \n",
    "            \n",
    "    elif k_face == 'top':\n",
    "        ## create DataArray for c point data, top\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','k_l','j','i'],\\\n",
    "                                coords={'time':(('time'),np.array([center_time]))})\n",
    "        tmp_da['k_l'].attrs['axis']  = 'Z'\n",
    "\n",
    "    ## name the array\n",
    "    tmp_da.name = da_name\n",
    "        \n",
    "    ## add additional coordinates to dataset\n",
    "    tmp_ds = tmp_da.to_dataset().assign_coords({\n",
    "        'time_bnds':time_bnds_da,\\\n",
    "        'XC':model_grid_ds.XC,\\\n",
    "        'YC':model_grid_ds.YC,\\\n",
    "        'XG':model_grid_ds.XG,\\\n",
    "        'YG':model_grid_ds.YG,\\\n",
    "        'XC_bnds':model_grid_ds.XC_bnds,\\\n",
    "        'YC_bnds':model_grid_ds.YC_bnds,\\\n",
    "        'Z':model_grid_ds.Z,\\\n",
    "        'Zu':model_grid_ds.Zu,\\\n",
    "        'Zl':model_grid_ds.Zl,\\\n",
    "        'Zp1':model_grid_ds.Zp1})\n",
    "        \n",
    " #   tmp_da = add_geo_metadata(tmp_da)\n",
    "\n",
    "    return tmp_ds\n",
    "\n",
    "\n",
    "def process_2D_variable(data_dir, filename, var_tmp_table, vars_table, sassie_n1_geometry_ds):\n",
    "    \n",
    "    var_name = var_tmp_table['variable'].values[0]\n",
    "    n_skip = var_tmp_table['field_index'].values[0] * 1\n",
    "    grid_point = var_tmp_table.cgrid_point.values\n",
    "    \n",
    "    ## process binary data to compact format\n",
    "    data_compact, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=1, skip=n_skip)\n",
    "    \n",
    "    ## convert compact format to 5 faces\n",
    "    data_faces = sassie_n1_compact_to_faces_2D(data_compact)\n",
    "    \n",
    "    ## convert faces to HHv2 Arctic rectangle\n",
    "    ## data on u and v points need to be handled differently from c points\n",
    "    if var_tmp_table.data_type.values == 'V': # if it is a vector field\n",
    "        var_mate = var_tmp_table.mate.values[0]\n",
    "        var_table_mate = vars_table[vars_table.variable.values == var_mate]\n",
    "        var_mate_field_index = var_table_mate.field_index.values[0]\n",
    "        \n",
    "        if grid_point == 'v':\n",
    "            ## get u field\n",
    "            n_skip_u = var_mate_field_index\n",
    "            data_compact_u, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=1, skip=n_skip_u)\n",
    "            face_arr_u = sassie_n1_compact_to_faces_2D(data_compact_u)\n",
    "            \n",
    "            ## process v field\n",
    "            var_HHv2 = combine_sassie_N1_faces_to_HHv2_2D_v_point(data_faces, face_arr_u)\n",
    "            \n",
    "        elif grid_point == 'u':\n",
    "            ## get v field\n",
    "            n_skip_v = var_mate_field_index\n",
    "            data_compact_v, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=1, skip=n_skip_v)\n",
    "            face_arr_v = sassie_n1_compact_to_faces_2D(data_compact_v)\n",
    "            \n",
    "            ## process u field\n",
    "            var_HHv2 = combine_sassie_N1_faces_to_HHv2_2D_u_point(data_faces, face_arr_v)\n",
    "            \n",
    "    elif var_tmp_table.data_type.values == 'S': # if it is a scalar field\n",
    "        var_HHv2 = combine_sassie_N1_faces_to_HHv2_2D(data_faces)\n",
    "    \n",
    "    ## add timestamp adn create dataset\n",
    "    timestamp = timestamp_from_iter_num(iter_num)\n",
    "    var_HHv2_ds = make_2D_HHv2_ds(var_HHv2, sassie_n1_geometry_ds, timestamp, grid_point=grid_point, da_name=var_name)\n",
    "\n",
    "    return var_HHv2_ds\n",
    "\n",
    "\n",
    "def process_3D_variable(data_dir, filename, var_tmp_table, vars_table, sassie_n1_geometry_ds):\n",
    "    \n",
    "    var_name = var_tmp_table['variable'].values[0]\n",
    "    \n",
    "    ## there are 90 vertical levels; use index from table to identify how many fields to skip\n",
    "    n_skip = var_tmp_table['field_index'].values[0] * 90\n",
    "    var_k_face = var_tmp_table['k_face'].values[0]\n",
    "    grid_point = var_tmp_table.cgrid_point.values\n",
    "    \n",
    "    ## process binary data to compact format\n",
    "    data_compact, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=90, skip=n_skip)\n",
    "    \n",
    "    ## convert compact format to 5 faces\n",
    "    data_faces = sassie_n1_compact_to_faces_3D(data_compact)\n",
    "    \n",
    "    ## convert faces to HHv2 Arctic rectangle\n",
    "    ## data on u and v points need to be handled differently from c points\n",
    "    if var_tmp_table.data_type.values == 'V': # if it is a vector field\n",
    "        var_mate = var_tmp_table.mate.values[0]\n",
    "        var_table_mate = vars_table[vars_table.variable.values == var_mate]\n",
    "        var_mate_field_index = var_table_mate.field_index.values[0]\n",
    "        \n",
    "        if grid_point == 'v':\n",
    "            ## get u field\n",
    "            n_skip_u = var_mate_field_index * 90\n",
    "            data_compact_u, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=90, skip=n_skip_u)\n",
    "            face_arr_u = sassie_n1_compact_to_faces_3D(data_compact_u)\n",
    "            \n",
    "            ## process v field\n",
    "            var_HHv2 = combine_sassie_N1_faces_to_HHv2_3D_v_point(data_faces, face_arr_u)\n",
    "            \n",
    "        elif grid_point == 'u':\n",
    "            ## get v field\n",
    "            n_skip_v = var_mate_field_index * 90\n",
    "            data_compact_v, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=90, skip=n_skip_v)\n",
    "            face_arr_v = sassie_n1_compact_to_faces_3D(data_compact_v)\n",
    "            \n",
    "            ## process u field\n",
    "            var_HHv2 = combine_sassie_N1_faces_to_HHv2_3D_u_point(data_faces, face_arr_v)\n",
    "        \n",
    "    elif var_tmp_table.data_type.values == 'S': # if it is a scalar field\n",
    "        var_HHv2 = combine_sassie_N1_faces_to_HHv2_3D(data_faces) # c point\n",
    "    \n",
    "    ## add timestamp and create dataset\n",
    "    timestamp = timestamp_from_iter_num(iter_num)\n",
    "    var_HHv2_ds = make_3D_HHv2_ds(var_HHv2, sassie_n1_geometry_ds, timestamp, grid_point=grid_point, da_name=var_name, k_face=var_k_face)\n",
    "    \n",
    "    return var_HHv2_ds\n",
    "\n",
    "\n",
    "def mask_dry_grid_cells(ds, var, geometry_ds, grid_point):\n",
    "    # .. I do not see why we need to copy the dataset instead of operating on it directly\n",
    "    ## make copy of dataset\n",
    "    #ds_tmp = ds.copy(deep=True)\n",
    " \n",
    "    ## tracer points use maskC, u points use maskW, and v points use maskS\n",
    "    if grid_point == 'c':\n",
    "        # some 3D vertical vector fields (like WVEL) \n",
    "        # use 'k_l' for their vertical dimension  \n",
    "        # to apply the wet/dry mask using maskC,\n",
    "        # we first have to temporarily rename the maskC \n",
    "        # vertical dimension from k to k_l \n",
    "        if 'k_l' in ds[var].dims:\n",
    "            ds[var] = ds[var].where(geometry_ds.maskC.rename({'k':'k_l'}) == True)\n",
    "        else: \n",
    "            ds[var] = ds[var].where(geometry_ds.maskC==True)\n",
    "        \n",
    "    elif grid_point == 'v':\n",
    "        ds[var] = ds[var].where(geometry_ds.maskS==True)\n",
    "        \n",
    "    elif grid_point == 'u':\n",
    "        ds[var] = ds[var].where(geometry_ds.maskW==True)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def create_encoding(ecco_ds, output_array_precision = np.float32):\n",
    "    \n",
    "    # Create NetCDF encoding directives\n",
    "    # ---------------------------------------------\n",
    "    # print('\\n... creating variable encodings')\n",
    "    # ... data variable encoding directives\n",
    "    \n",
    "    # Define fill values for NaN\n",
    "    if output_array_precision == np.float32:\n",
    "        netcdf_fill_value = nc4.default_fillvals['f4']\n",
    "\n",
    "    elif output_array_precision == np.float64:\n",
    "        netcdf_fill_value = nc4.default_fillvals['f8']\n",
    "    \n",
    "    dv_encoding = dict()\n",
    "    for dv in ecco_ds.data_vars:\n",
    "        dv_encoding[dv] =  {'compression':'zlib',\\\n",
    "                            'complevel':5,\\\n",
    "                            'shuffle':False,\\\n",
    "                            'fletcher32': False,\\\n",
    "                            '_FillValue':netcdf_fill_value}\n",
    "\n",
    "    # ... coordinate encoding directives\n",
    "    coord_encoding = dict()\n",
    "    \n",
    "    for coord in ecco_ds.coords:\n",
    "        # set default no fill value for coordinate\n",
    "        if output_array_precision == np.float32:\n",
    "            coord_encoding[coord] = {'_FillValue':None, 'dtype':'float32'}\n",
    "        elif output_array_precision == np.float64:\n",
    "            coord_encoding[coord] = {'_FillValue':None, 'dtype':'float64'}\n",
    "\n",
    "        # force 64 bit ints to be 32 bit ints\n",
    "        if (ecco_ds[coord].values.dtype == np.int32) or \\\n",
    "           (ecco_ds[coord].values.dtype == np.int64) :\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "        # fix encoding of time\n",
    "        if coord == 'time' or coord == 'time_bnds':\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "            if 'units' in ecco_ds[coord].attrs:\n",
    "                # apply units as encoding for time\n",
    "                coord_encoding[coord]['units'] = ecco_ds[coord].attrs['units']\n",
    "                # delete from the attributes list\n",
    "                del ecco_ds[coord].attrs['units']\n",
    "\n",
    "        elif coord == 'time_step':\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "    # ... combined data variable and coordinate encoding directives\n",
    "    encoding = {**dv_encoding, **coord_encoding}\n",
    "\n",
    "    return encoding\n",
    "\n",
    "\n",
    "def modify_metadata(ds, var, var_filename_netcdf):   \n",
    "    title = 'SASSIE Ocean Model ' + var + ' Parameter for the Lat-Lon-Cap 1080 (llc1080) Native Model Grid (Version 1 Release 1)'\n",
    "  \n",
    "    ## edit specific metadata for these datasets\n",
    "    ds.attrs['author'] = 'Mike Wood, Marie Zahn, and Ian Fenty'\n",
    "    ds.attrs['comment'] = 'SASSIE llc1080 V1R1 fields are consolidated onto a single curvilinear grid face focusing on the Arctic domain using fields from the 5 faces of the lat-lon-cap 1080 (llc1080) native grid used in the original simulation.'\n",
    "    ds.attrs['id'] = '10.5067/XXXXX-XXXXX' # will update with DOI when avail\n",
    "\n",
    "    if 'k' in list(ds.dims):\n",
    "        ds.attrs['geospatial_vertical_min'] = np.round(ds.Zu.min().values,1)\n",
    "        \n",
    "    ds.attrs['geospatial_lat_min'] = np.round(ds.YC.min().values,1)\n",
    "    ds.attrs['metadata_link'] = 'https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=TBD' # will update with DOI when avail\n",
    "    ds.attrs['product_name'] = var_filename_netcdf\n",
    "    ds.attrs['time_coverage_end'] = str(ds.time_bnds.values[0][0])[:-10]\n",
    "    ds.attrs['time_coverage_start'] = str(ds.time_bnds.values[0][1])[:-10]\n",
    "    ds.attrs['product_version'] = 'Version 1, Release 1'\n",
    "    ds.attrs['program'] = 'NASA Physical Oceanography'\n",
    "    ds.attrs['source'] = 'The SASSIE ocean model simulation was produced by downscaling the global ECCO state estimate from 1/3 to 1/12 degree grid cells. The ECCO global solution provided initial and boundary conditions and atmospheric forcing.'\n",
    "    ds.attrs['references'] = 'TBD'\n",
    "    ds.attrs['summary'] = 'This dataset provides data variable and geometric parameters for the lat-lon-cap 1080 (llc1080) native model grid from the SASSIE ECCO ocean model Version 1 Release 1 (V1r1) ocean and sea-ice state estimate.'\n",
    "    ds.attrs['title'] = title\n",
    "    ds.attrs['uuid'] = str(uuid.uuid1())\n",
    "    ds.attrs['history'] ='Initial release of the ECCO N1 Sassie Ocean-Sea Ice Simulation'\n",
    "    \n",
    "    ## remove some attributes we don't need\n",
    "    attributes_to_remove = ['product_time_coverage_start', 'product_time_coverage_end',\\\n",
    "                            'geospatial_lat_resolution', 'geospatial_lon_resolution']\n",
    "    \n",
    "    ## add current time and date\n",
    "    current_time = datetime.now().isoformat()[0:19]\n",
    "    ds.attrs['date_created'] = current_time\n",
    "    ds.attrs['date_modified'] = current_time\n",
    "    ds.attrs['date_metadata_modified'] = current_time\n",
    "    ds.attrs['date_issued'] = current_time\n",
    "    \n",
    "    for attr in attributes_to_remove:\n",
    "        ds.attrs.pop(attr, None)\n",
    "        \n",
    "    return ds\n",
    "\n",
    "def reorder_dims(xr_dataset):\n",
    "    ## specify order of dims\n",
    "\n",
    "    # if 3D\n",
    "    if 'k' in list(xr_dataset.dims):\n",
    "        #tmp = xr_dataset[[\"time\",\"j\",\"i\",\"k\",\"j_g\",\"i_g\",\"k_u\",\"k_l\",\"k_p1\",\"nv\",\"nb\"]]\n",
    "        tmp = xr_dataset[[\"time\",\"k\",\"j\",\"i\",\"k_u\",\"k_l\",\"k_p1\",\"nv\"]]\n",
    "    # if 2D\n",
    "    else:\n",
    "        tmp = xr_dataset[[\"time\",\"j\",\"i\",\"nv\"]]\n",
    "\n",
    "    tmp = tmp.drop_indexes([\"nv\"]).reset_coords([\"nv\"], drop=True)\n",
    "    \n",
    "\n",
    "    ## reassign dataset to new dims\n",
    "    xr_ds_ordered = tmp.assign(xr_dataset)\n",
    "    \n",
    "    return xr_ds_ordered\n",
    "\n",
    "def push_nc_dir_to_ec2(nc_root_dir_ec2, root_dest_s3_name):\n",
    "    \"\"\"\n",
    "    Pushes the netcdf files from a directory to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        nc_root_dir_ec2 (str): The root directory containing the netcdf files on the EC2 instance.\n",
    "        root_dest_s3_name (str): The root name of the S3 bucket where the files will be pushed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ## upload each subdirectory in the nc root directory\n",
    "    \n",
    "    # Get a list of all items (files and folders) in the directory\n",
    "    all_subdirs = os.listdir(nc_root_dir_ec2)\n",
    "\n",
    "    for subdir_name in all_subdirs:\n",
    "        if subdir != '.ipynb_checkpoints':\n",
    "            print(subdir)\n",
    "            \n",
    "            \n",
    "            ## push file to s3 bucket\n",
    "            mybucket = root_dest_s3_name + subdir_name\n",
    "            subdir_fullpath = Path(nc_root_dir_ec2 / subdir_name)\n",
    "            nc_files = list(subdir_fullpath.glob('*.nc'))\n",
    "    \n",
    "            print(f'\\n>pushing netcdf files in {subdir_name} to s3 bucket : {mybucket}')\n",
    "            print(f'... looking for *.nc files in {subdir_name}')\n",
    "            print(f'... found {len(nc_files)} nc files to upload')\n",
    "    \n",
    "            if len(nc_files)>0:\n",
    "                cmd=f\"aws s3 cp {subdir_fullpath} {mybucket}/ --recursive --include '*.nc' --no-progress > /dev/null 2>&1\"\n",
    "                print(f'... aws command: {cmd}')\n",
    "                with suppress_stdout():\n",
    "                   os.system(cmd)\n",
    "            else:\n",
    "                print(\"... nothing to upload!\") \n",
    "            \n",
    "        else:\n",
    "            print(f\"No subdirectories found in {nc_root_dir_ec2}\")\n",
    "\n",
    "def save_sassie_netcdf_to_ec2(var_HHv2_ds, nc_dir_ec2, var_filename_netcdf):\n",
    "    ## save netCDF files to disk and then push to s3 bucket\n",
    "\n",
    "    # var_HHv2_ds         : The xarray dataset that will be saved to netcdf\n",
    "    # nc_dir_ec2          : The directory where the netcdf file will be saved on the ec2 instance\n",
    "    # var_filename_netcdf : The name of the netcdf file that will be saved\n",
    "    \n",
    "    ## create encoding\n",
    "    encoding_var = create_encoding(var_HHv2_ds, output_array_precision = np.float32)\n",
    "    \n",
    "    ## stage netcdf on tmp directory on ec2\n",
    "    tmp_netcdf_filename = nc_dir_ec2 / var_filename_netcdf\n",
    "\n",
    "    print('   saving netcdf to ', tmp_netcdf_filename)\n",
    "    var_HHv2_ds.to_netcdf(str(tmp_netcdf_filename), encoding = encoding_var)\n",
    "    var_HHv2_ds.close()\n",
    "        \n",
    "\n",
    "def plot_sassie_HHv2_3D(face_arr, depth_level=0, vmin=None, vmax=None,\\\n",
    "    cmap='jet', axs = None, \\\n",
    "    show_colorbar=True):\n",
    "\n",
    "    tmp = combine_sassie_N1_faces_to_HHv2_3D(face_arr)\n",
    "\n",
    "    if vmin == None:\n",
    "        vmin = np.min(tmp)\n",
    "    if vmax == None:\n",
    "        vmax = np.max(tmp)\n",
    "\n",
    "    if axs == None:\n",
    "        plt.imshow(tmp[depth_level,:,:], origin='lower', interpolation='none',vmin=vmin,vmax=vmax, cmap=cmap)\n",
    "        if show_colorbar:\n",
    "            plt.colorbar()\n",
    "\n",
    "    else:\n",
    "        im1 = axs.imshow(tmp[depth_level,:,:], origin='lower', interpolation='none',vmin=vmin,vmax=vmax, cmap=cmap)\n",
    "        fig = plt.gcf()\n",
    "        if show_colorbar:\n",
    "            fig.colorbar(im1, ax=axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "162f6243-c02f-4bb3-b051-82092bbc8dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_HH_netcdfs(data_filename, data_dir_ec2, nc_root_dir_ec2, metadata_dict, sassie_n1_geometry_ds, vars_table, save_nc_to_disk):\n",
    "    \"\"\"\n",
    "    Create netCDF files for all variables in a given file.\n",
    "\n",
    "    Args:\n",
    "        data_filename (str): The name *.data filename.\n",
    "        data_dir_ec2 (str): The directory containing the data files.\n",
    "        nc_root_dir_ec2 (str): The root directory where all netCDF files are saved.\n",
    "        metadata_dict (dict): A dictionary containing metadata information.\n",
    "        sassie_n1_geometry_ds (Dataset): The dataset containing geometry information.\n",
    "        vars_table (DataFrame): A table containing variable information.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    ## loop through each variable that was requested --------------------------------------------\n",
    "    \n",
    "    print('\\n############ processing:', data_filename, '############')\n",
    "    \n",
    "    ## identify variables in this dataset\n",
    "    meta_file_path = data_dir_ec2 + \"/\" + data_filename[:-5] + \".meta\"\n",
    "    meta_file_dict = MITgcmutils.mds.parsemeta(meta_file_path)\n",
    "    vars_in_dataset = meta_dict['fldList']\n",
    "    \n",
    "    for variable in vars_in_dataset:\n",
    "        \n",
    "        ## create directory for variable\n",
    "        nc_dir_ec2 = nc_root_dir_ec2 + \"/\" + variable + \"_AVG_DAILY\"\n",
    "        try:\n",
    "            nc_dir_ec2.mkdir(exist_ok=True, parents=True)\n",
    "        except :\n",
    "            print(f\"ERROR: could not make {nc_dir_ec2} \")\n",
    "            exit()\n",
    "        \n",
    "        ## get variable data from table\n",
    "        var_tmp_table = vars_table[vars_table.variable.isin([variable])]\n",
    "        print('>> processing: ', variable)\n",
    "        \n",
    "        ## 3D data processing\n",
    "        if var_tmp_table['n_dims'].values == '3D':\n",
    "            ## process dataset\n",
    "            var_HHv2_ds = process_3D_variable(data_dir_ec2, data_filename, var_tmp_table,\\\n",
    "                                              vars_table, sassie_n1_geometry_ds)\n",
    "                                              ## mask land cells\n",
    "                \n",
    "            #print('\\nds after 3d loading')\n",
    "            #show_me_the_ds(var_HHv2_ds)\n",
    "\n",
    "            ## mask land cells\n",
    "            var_HHv2_ds = mask_dry_grid_cells(var_HHv2_ds, var_name, sassie_n1_geometry_ds, \\\n",
    "                                              grid_point=var_tmp_table['cgrid_point'].values)\n",
    "            #print('\\nds after 3d masking:')\n",
    "\n",
    "        ## 2D data processing \n",
    "        elif var_tmp_table['n_dims'].values == '2D':\n",
    "            ## process dataset\n",
    "            var_HHv2_ds = process_2D_variable(data_dir_ec2, data_filename, var_tmp_table,\\\n",
    "                                              vars_table, sassie_n1_geometry_ds)\n",
    "            #print('\\nds after 2d loading')\n",
    "            #show_me_the_ds(var_HHv2_ds)\n",
    "\n",
    "            ## mask land cells\n",
    "            # select out the k=0 level of the geometry dataset\n",
    "            var_HHv2_ds = mask_dry_grid_cells(var_HHv2_ds, var_name, sassie_n1_geometry_ds.isel(k=0), \\\n",
    "                                              grid_point=var_tmp_table['cgrid_point'].values)\n",
    "            \n",
    "            # 2D datasets have neither k dims nor Z coordinates\n",
    "            var_HHv2_ds = var_HHv2_ds.drop_vars(['k','Z'])\n",
    "            #print('\\nds after 2d masking:')\n",
    "            #show_me_the_ds(var_HHv2_ds)\n",
    "\n",
    "        # drop grid cell corner dims and coordinates\n",
    "        var_HHv2_ds = var_HHv2_ds.drop_vars(['i_g','j_g','XG','YG','XC_bnds','YC_bnds'])\n",
    "\n",
    "        ## add metadata\n",
    "        global_latlon_metadata = metadata_dict['ECCOv4r4_global_metadata_for_all_datasets'] + metadata_dict['ECCOv4r4_global_metadata_for_latlon_datasets']\n",
    "        var_HHv2_ds = ecco.add_global_metadata(global_latlon_metadata, var_HHv2_ds, var_tmp_table['n_dims'].values[0])\n",
    "        var_HHv2_ds = ecco.add_coordinate_metadata(metadata_dict['ECCOv4r4_coordinate_metadata_for_latlon_datasets'], var_HHv2_ds, less_output=True)\n",
    "        var_HHv2_ds, grouping_keywords = ecco.add_variable_metadata(metadata_dict['ECCOv4r4_geometry_metadata_for_latlon_datasets'], var_HHv2_ds, less_output=True)\n",
    "        var_HHv2_ds, grouping_keywords = ecco.add_variable_metadata(metadata_dict['ECCOv4r4_variable_metadata'], var_HHv2_ds, less_output=True)\n",
    "        \n",
    "        ## generate filename\n",
    "        center_time = var_HHv2_ds.time.values\n",
    "        yyyy_mm_dd = str(center_time)[2:6] + \"-\" + str(center_time)[7:9] + \"-\" + str(center_time)[10:12]\n",
    "        var_filename_netcdf = var_name + \"_day_mean_\" + yyyy_mm_dd + \"_ECCO_SASSIE_V1_HH_llc1080.nc\"\n",
    "        \n",
    "        ## tweak some of the global attributes\n",
    "        var_HHv2_ds = modify_metadata(var_HHv2_ds, var_name, var_filename_netcdf)\n",
    "        \n",
    "        ## reorder dims\n",
    "        var_HHv2_ds = reorder_dims(var_HHv2_ds)\n",
    "        \n",
    "        #data_dir_ec2\n",
    "        ## save netcdf\n",
    "        if save_nc_to_disk:\n",
    "            save_sassie_netcdf_to_ec2(var_HHv2_ds, nc_dir_ec2, var_filename_netcdf)\n",
    "        \n",
    "        # remove from memory\n",
    "        var_HHv2_ds = None\n",
    "\n",
    "    # return(var_HHv2_ds_final)\n",
    "    print(\"######## processing complete ########\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45a68e18-4670-401f-859e-2c2055e62e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Create final routine to process files ########### \n",
    "\n",
    "## Specify root directory and process all variables in that dataset\n",
    "\n",
    "def generate_sassie_ecco_netcdfs(root_filenames, root_s3_name, root_dest_s3_name, \n",
    "                                 force_redownload, keep_local_files, push_to_s3,\n",
    "                                 files_to_process, local_scratch_dir, save_nc_to_disk):\n",
    "    \n",
    "\n",
    "    ## get list of gz files in s3 directory\n",
    "    s3 = []\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "        \n",
    "    ## --------------------------------------------\n",
    "    ## open model geometry from ec2\n",
    "\n",
    "    n1_geometry_local_dir =  Path('/home/jpluser/sassie/N1/GRID_GEOMETRY')\n",
    "    n1_geometry_local_filename = 'GRID_GEOMETRY_SASSIE_HH_V1R1_NATIVE_LLC1080.nc'\n",
    "    n1_geometry_local_full_path = n1_geometry_local_dir / n1_geometry_local_filename\n",
    "    n1_geometry_s3_url = 's3://ecco-processed-data/SASSIE/N1/V1/HH/NETCDF/GRID/GRID_GEOMETRY_SASSIE_HH_V1R1_NATIVE_LLC1080.nc'\n",
    "\n",
    "    print('> loading geometry dataset')\n",
    "    if not n1_geometry_local_full_path.is_file(): # checks if file is there\n",
    "        # make geometry directory\n",
    "        try:\n",
    "            print(f\"...creating geometry directory {n1_geometry_local_dir}\")\n",
    "            n1_geometry_local_dir.mkdir(exist_ok=False, parents=True)\n",
    "        except FileExistsError:\n",
    "            print(f\"...directory {n1_geometry_local_dir} already exists\")\n",
    "\n",
    "        # download geometry file\n",
    "        try:\n",
    "            s3.download(n1_geometry_s3_url, str(n1_geometry_local_full_path))\n",
    "            print(f\"...downloaded {n1_geometry_s3_url} to {n1_geometry_local_full_path}\")\n",
    "        except:\n",
    "            print(f\"ERROR: could not download {n1_geometry_s3_url} to {n1_geometry_local_full_path}\")\n",
    "            return\n",
    "\n",
    "    sassie_n1_geometry_ds = xr.open_dataset(n1_geometry_local_full_path)\n",
    "    sassie_n1_geometry_ds.close()\n",
    "    print('... geometry file loaded')\n",
    "    \n",
    "    ## open table that includes metadata for all variables\n",
    "    vars_table = pd.read_csv('/home/jpluser/git_repos/SASSIE_downscale_ecco_v5/sassie_variables_table.csv', index_col=False)\n",
    "    \n",
    "    ## --------------------------------------------\n",
    "    ## load metadata \n",
    "    metadata_json_dir = '/home/jpluser/git_repos/ECCO-ACCESS/metadata/ECCOv4r4_metadata_json/'\n",
    "    \n",
    "    metadata_fields = ['ECCOv4r4_global_metadata_for_all_datasets',\n",
    "                       'ECCOv4r4_global_metadata_for_latlon_datasets',\n",
    "                       'ECCOv4r4_global_metadata_for_native_datasets',\n",
    "                       'ECCOv4r4_coordinate_metadata_for_1D_datasets',\n",
    "                       'ECCOv4r4_coordinate_metadata_for_latlon_datasets',\n",
    "                       'ECCOv4r4_coordinate_metadata_for_native_datasets',\n",
    "                       'ECCOv4r4_geometry_metadata_for_latlon_datasets',\n",
    "                       'ECCOv4r4_geometry_metadata_for_native_datasets',\n",
    "                       'ECCOv4r4_groupings_for_1D_datasets',\n",
    "                       'ECCOv4r4_groupings_for_latlon_datasets',\n",
    "                       'ECCOv4r4_groupings_for_native_datasets',\n",
    "                       'ECCOv4r4_variable_metadata',\n",
    "                       'ECCOv4r4_variable_metadata_for_latlon_datasets',\n",
    "                       'ECCOv4r4_dataset_summary']\n",
    "    \n",
    "    ## load metadata\n",
    "    metadata_dict = dict()\n",
    "    \n",
    "    for mf in metadata_fields:\n",
    "        mf_e = mf + '.json'\n",
    "        # print(mf_e)\n",
    "        with open(Path(metadata_json_dir + mf_e), 'r') as fp:\n",
    "            metadata_dict[mf] = json.load(fp)\n",
    "    \n",
    "    ## --------------------------------------------\n",
    "    ## loop through gz files in root directory and process all variables included in the dataset\n",
    "    \n",
    "    # find filenames\n",
    "    file_list = np.sort(s3.glob(f'{root_s3_name}{root_filenames}/*tar.gz'))\n",
    "\n",
    "    print(f'\\n> Looking for files on {root_s3_name}{root_filenames}')\n",
    "    print(f'... num files  : {len(file_list)}')\n",
    "    print(f'... first file : {file_list[0]}')\n",
    "    print(f'... last file  : {file_list[-1]}')\n",
    "      \n",
    "    # construct url form of filenames\n",
    "    data_urls = ['s3://' + f for f in file_list ]\n",
    "\n",
    "    print(f'\\n> Preparing list of files to process')\n",
    "    ## specify start and end indices or process all files   \n",
    "    if len(files_to_process) == 2: # two numbers indicates a range (two indices)\n",
    "        data_urls_select = data_urls[files_to_process[0]:files_to_process[1]]\n",
    "        print(f'... first file to process : {data_urls_select[0]}')\n",
    "        print(f'... last file to process  : {data_urls_select[-1]}')\n",
    "    \n",
    "    elif len(files_to_process) == 1 and files_to_process[0] == -1: # process all files\n",
    "        data_urls_select = data_urls\n",
    "        print(f'... first file to process : {data_urls_select[0]}')\n",
    "        print(f'... last file to process  : {data_urls_select[-1]}')\n",
    "    \n",
    "    elif len(files_to_process) == 1 and files_to_process[0] >= 0: # process one file using number as index\n",
    "        # wrap in list\n",
    "        data_urls_select = [data_urls[files_to_process[0]]]\n",
    "        print(f'... 1 file to process : {data_urls_select}')\n",
    "    \n",
    "    else:\n",
    "        print(\"ERROR: invalid entry for `files_to_process` argument\")\n",
    "        return \n",
    " \n",
    "\n",
    "    # Loop through all data_urls\n",
    "    for data_url in data_urls_select:\n",
    "        \n",
    "        ## download tar.gz file from s3 cloud to ec2 tmp_dir\n",
    "        gz_filename = data_url.split(\"/\")[-1]\n",
    "\n",
    "        print(f'\\n==== processing file: {gz_filename} ====')\n",
    "        gz_tmp_dir_base = f\"{gz_filename.split('.')[0]}_{gz_filename.split('.')[1]}\"\n",
    "\n",
    "        gz_dir_ec2 =  Path(f\"{local_scratch_dir}/tmp_gz/{gz_tmp_dir_base}\")\n",
    "        nc_root_dir_ec2 =  Path(f\"{local_scratch_dir}/tmp_nc/{gz_tmp_dir_base}\")\n",
    "\n",
    "        print(f'... temporary gz directory {gz_dir_ec2}')\n",
    "        print(f'... temporary nc directory {nc_root_dir_ec2}')\n",
    "\n",
    "        gz_dir_ec2.mkdir(exist_ok=True, parents=True)\n",
    "        nc_root_dir_ec2.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        gz_full_path = gz_dir_ec2 / gz_filename\n",
    "\n",
    "        print('\\n> preparing *data file for processing')\n",
    "        print(f'... gz file: {gz_full_path}')\n",
    "       \n",
    " \n",
    "        # download gz file\n",
    "        if (not gz_full_path.is_file()) or (force_redownload): # checks if file is there\n",
    "            start_time_a = time.time()\n",
    "            print(f'... gz file needs to be downloaded: isfile()={gz_full_path.is_file()} or force_redownload: {force_redownload}')\n",
    "            s3.download(data_url, str(gz_full_path))\n",
    "            print('... download time ', time.time() - start_time_a)\n",
    "        else: \n",
    "            print(f'... not re-downloading gz because local gz is present: isfile()={gz_full_path.is_file()}')\n",
    "            \n",
    "        ## decompress tar.gz file into *.data and *.meta files\n",
    "        unpack_tar_gz_files(gz_dir_ec2, keep_local_files)\n",
    "    \n",
    "        ## loop through each *.data file stored in the gz_dir_ec2 directory and process all variables within it\n",
    "        ## create DataSets for each variable type within the *.data files\n",
    "        ## and possibly store to disk if 'save_nc_to_disk==True'\n",
    "        data_files = np.sort(list(gz_dir_ec2.glob('*.data')))\n",
    "        \n",
    "        if len(data_files) == 0:\n",
    "           print('... no data files to process in {data_dir_ec2}')   \n",
    "           return\n",
    "        else:\n",
    "           print(f'found {len(data_files)} *.data files in {gz_dir_ec2}')  \n",
    "        \n",
    "        for data_file in data_files:\n",
    "            \n",
    "            ## process all variables in the dataset stored in the gz_dir_ec2 directory\n",
    "            create_HH_netcdfs(data_file, gz_dir_ec2, nc_root_dir_ec2, metadata_dict, sassie_n1_geometry_ds, vars_table, save_nc_to_disk)\n",
    "        \n",
    "        ## after processing is complete, delete data files on ec2\n",
    "        ## push nc files to aws s3\n",
    "        if push_to_s3:\n",
    "            push_nc_dir_to_ec2(nc_root_dir_ec2, root_dest_s3_name)\n",
    "        else:\n",
    "            print('> not pushing files to s3')\n",
    "    \n",
    "        print('\\n> cleaning up local nc files') \n",
    "        if keep_local_files:\n",
    "            print('... keeping local nc  directories')\n",
    "        else:\n",
    "            ## remove tmp nc var directory and all of its contents\n",
    "            print(\"... removing tmp nc dir \", nc_dir_ec2)\n",
    "            os.system(f\"rm -rf {nc_dir_ec2}\")\n",
    "        \n",
    "        \n",
    "        print('\\n> cleaning up local gz directories')\n",
    "        if keep_local_files:\n",
    "            print('... keeping local gz directories')\n",
    "        else:\n",
    "            ## remove tmp tar.gz files\n",
    "            print(\"... removing tmp gz dir\")\n",
    "            os.system(f\"rm -rf {str(gz_dir_ec2)}\")\n",
    "\n",
    "            print(\"... removing tmp nc root dir \", nc_root_dir_ec2)\n",
    "            os.system(f\"rm -rf {str(nc_root_dir_ec2)}\")\n",
    "\n",
    "        print(f'\\n==== done processing file: {gz_filename} ====\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97efd377-d1ba-4481-a888-feb3ece90539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
