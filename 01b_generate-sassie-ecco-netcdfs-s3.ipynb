{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e27f266-b8f4-4b04-84db-74102e7eea65",
   "metadata": {},
   "source": [
    "# Process SASSIE ocean model granules on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c84f351-0d07-465d-9a4c-7d3910f915b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import netCDF4 as nc4\n",
    "import tarfile\n",
    "import json\n",
    "import uuid as uuid\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import s3fs\n",
    "import argparse\n",
    "\n",
    "## import ECCO utils\n",
    "import sys\n",
    "sys.path.append('/Users/mzahn/github_others/ECCOv4-py')\n",
    "import ecco_v4_py as ecco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfaeed1d-3bd1-4e7e-93f5-bd0836682a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sassie_N1_field(file_dir, fname, nk=1, skip=0):\n",
    "    num_cols = 680*4 + 1080\n",
    "    num_rows = 1080\n",
    "    \n",
    "    time_level = int(fname.split('.data')[0].split('.')[-1])\n",
    "    \n",
    "    tmp_compact = ecco.load_binary_array(file_dir, fname, \\\n",
    "                                    num_rows, num_cols, nk=nk, skip=skip, filetype='>f4')\n",
    "\n",
    "    return tmp_compact, time_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e3fd3a-ddfa-4d1a-8932-2c9a78c43f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sassie_n1_compact_to_faces_2D(sassie_n1_compact):\n",
    "    sassie_faces = dict()\n",
    "    n = 680\n",
    "    \n",
    "    # Face 1 \n",
    "    start_row = 0\n",
    "    end_row = n\n",
    "    sassie_faces[1] = sassie_n1_compact[start_row:end_row,:]\n",
    "\n",
    "    # Face 2\n",
    "    start_row = end_row\n",
    "    end_row = start_row + n\n",
    "\n",
    "    sassie_faces[2] = sassie_n1_compact[start_row:end_row,:]\n",
    "    \n",
    "    # Face 3\n",
    "    start_row = end_row\n",
    "    end_row = start_row + 1080\n",
    "    sassie_faces[3] = sassie_n1_compact[start_row:end_row:,:]\n",
    "    \n",
    "    #Face 4\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[4] = sassie_n1_compact[start_row:end_row].reshape(1080, n)\n",
    "\n",
    "    #Face 5\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[5] = sassie_n1_compact[start_row:end_row].reshape(1080, n)\n",
    "\n",
    "    return sassie_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09477f84-8112-4716-b74c-42c741039eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sassie_n1_compact_to_faces_3D(sassie_n1_compact):\n",
    "    sassie_faces = dict()\n",
    "    n = 680\n",
    "    \n",
    "    # Face 1 \n",
    "    start_row = 0\n",
    "    end_row = n\n",
    "    sassie_faces[1] = sassie_n1_compact[:,start_row:end_row,:]\n",
    "\n",
    "    # Face 2\n",
    "    start_row = end_row\n",
    "    end_row = start_row + n\n",
    "    sassie_faces[2] = sassie_n1_compact[:,start_row:end_row,:]\n",
    "    \n",
    "    # Face 3\n",
    "    start_row = end_row\n",
    "    end_row = start_row + 1080\n",
    "    sassie_faces[3] = sassie_n1_compact[:,start_row:end_row:,:]\n",
    "    \n",
    "    #Face 4\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[4] = sassie_n1_compact[:,start_row:end_row].reshape(90, 1080, n)\n",
    "\n",
    "    #Face 5\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[5] = sassie_n1_compact[:,start_row:end_row].reshape(90, 1080, n)\n",
    "\n",
    "    return sassie_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75aa6e3f-1790-4db9-b0d2-0f58b63f82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sassie_N1_faces_to_HHv2_2D(face_arr):\n",
    "    \"\"\"\n",
    "    2D function for scalar fields, c point\n",
    "    \"\"\"\n",
    "    # dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((1080, 1800)) \n",
    "    \n",
    "    # cut out sections we want and assign them to location on HH\n",
    "    new_arr[:, 185:185 + 1080] = face_arr[3]\n",
    "    \n",
    "    # rotate Face 1 to line up with orientation of Face 3\n",
    "    new_arr[:, 0:185] = np.flipud(face_arr[1][-185:,:].T) # flip and transpose\n",
    "    \n",
    "    new_arr[:, 185 + 1080:] = face_arr[4][:,:535]\n",
    "\n",
    "    new_arr = np.rot90(new_arr,2) # rotate it 180 so Greenland/AK are on bottom\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71a97ee9-dbc0-4756-83c9-614b21be9ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sassie_N1_faces_to_HHv2_2D_u_point(face_arr_u, face_arr_v):\n",
    "    \"\"\"\n",
    "    2D function for vector fields, u point\n",
    "    \"\"\"\n",
    "    ## dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((1080, 1800))\n",
    "    \n",
    "    ## add Arctic face (3)\n",
    "    new_arr[:, 185:185+1080] = face_arr_u[3] # take entire Artic face\n",
    "    \n",
    "    ## add face 1 that will be flipped (must use v array)\n",
    "    new_arr[:, 0:185] = np.flipud(face_arr_v[1][-185:,:].T)\n",
    "        \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:, 185+1080:] = face_arr_u[4][:,:535]\n",
    "\n",
    "    ## rotate by 90 deg twice to have Alaska on bottom left\n",
    "    ## since it is vector field, have to multiply whole array by -1\n",
    "    new_arr = np.rot90(new_arr,2)\n",
    "    new_arr = new_arr *-1\n",
    "        \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ceb0bb0-60b7-43ef-95dd-8526abdfb4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sassie_N1_faces_to_HHv2_2D_v_point(face_arr_v, face_arr_u, vec=False):\n",
    "    \"\"\"\n",
    "    2D function for vector fields, v point\n",
    "    \"\"\"\n",
    "    ## dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((1080, 1800))\n",
    "    \n",
    "    ## add Arctic face (3)\n",
    "    new_arr[:, 185:185+1080] = face_arr_v[3]\n",
    "    \n",
    "    ## add part of face 1 (Europe) that will be flipped (must use u array and multiply by -1)\n",
    "    \n",
    "    ## after rotating face 1, the u points on face 1 will not match the v points of face 3 (offset by 1 upwards)\n",
    "    ## therefore, must remove the first column of face 1 and add the first column from face 2 to the end of face 1\n",
    "    ## remove the first i column from the u field so the shape is (680, 1079) = (j,i)\n",
    "    face1_tmp = face_arr_u[1][:,1:]\n",
    "    ## then add the first row from face 2 to the end of face 1\n",
    "    face1_mod = np.concatenate((face1_tmp, face_arr_u[2][:,:1]), axis=1)\n",
    "    \n",
    "    ## add modified face 1 by rotating and multiplying by -1\n",
    "    new_arr[:, 0:185] = np.flipud(face1_mod[-185:,:].T)*-1\n",
    "    \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:, 185+1080:] = face_arr_v[4][:,:535]\n",
    "\n",
    "    ## rotate by 90 deg twice to have Alaska on bottom left\n",
    "    new_arr = np.rot90(new_arr,2)\n",
    "    new_arr = new_arr *-1\n",
    "        \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "194e7405-099a-42ea-9b21-57a5a38d0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sassie_N1_faces_to_HHv2_3D(face_arr):\n",
    "    \"\"\"\n",
    "    3D function for scalar fields, c point\n",
    "    \"\"\"\n",
    "    # dimensions of the final Arctic HH field. 535+185+1080=1800 ; 90 vertical levels\n",
    "    new_arr = np.zeros((90, 1080, 1800)) \n",
    "    \n",
    "    # cut out sections we want and assign them to location on HH\n",
    "    new_arr[:,:,185:185+1080] = face_arr[3]\n",
    "    \n",
    "    # rotate Face 1 to line up with orientation of Face 3\n",
    "    new_arr[:,:,0:185] = np.transpose(face_arr[1][:,-185:,::-1],axes=(0,2,1)) # flip and transpose\n",
    "    \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:,:,185+1080:] = face_arr[4][:,:,:535]\n",
    "    \n",
    "    ## rotate it 180 so Greenland/AK are on bottom\n",
    "    new_arr = np.rot90(new_arr,2,axes=(1,2)) \n",
    "    \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88047243-9846-466f-acd6-19abd28eb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sassie_N1_faces_to_HHv2_3D_u_point(face_arr_u, face_arr_v):\n",
    "    \"\"\"\n",
    "    3D function for vector fields, u point\n",
    "    \"\"\"\n",
    "    ## dimensions of the final Arctic HH field. 535+185+1080=1800 ; 90 vertical levels\n",
    "    new_arr = np.zeros((90, 1080, 1800))\n",
    "    \n",
    "    ## add Arctic face (3)\n",
    "    new_arr[:,:,185:185+1080] = face_arr_u[3] # take entire Artic face\n",
    "    \n",
    "    ## add face 1 that will be flipped (must use v array)\n",
    "    new_arr[:,:,0:185] = np.transpose(face_arr_v[1][:,-185:,::-1],axes=(0,2,1))\n",
    "        \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:,:,185+1080:] = face_arr_u[4][:,:,:535]\n",
    "\n",
    "    ## rotate by 90 deg twice to have Alaska on bottom left\n",
    "    ## since it is vector field, have to multiply whole array by -1\n",
    "    new_arr = np.rot90(new_arr,2,axes=(1,2))\n",
    "    new_arr = new_arr *-1\n",
    "        \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1662b433-af58-478b-b90d-60a003fcc3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sassie_N1_faces_to_HHv2_3D_v_point(face_arr_v, face_arr_u):\n",
    "    \"\"\"\n",
    "    3D function for vector fields, v point\n",
    "    \"\"\"\n",
    "    ## dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((90, 1080, 1800))\n",
    "    \n",
    "    ## add Arctic face (3)\n",
    "    new_arr[:,:,185:185+1080] = face_arr_v[3]\n",
    "    \n",
    "    ## add part of face 1 (Europe) that will be flipped (must use u array and multiply by -1)\n",
    "    \n",
    "    ## after rotating face 1, the u points on face 1 will not match the v points of face 3 (offset by 1 upwards)\n",
    "    ## therefore, must remove the first column of face 1 and add the first column from face 2 to the end of face 1\n",
    "    ## remove the first i column from the u field so the shape is (680, 1079) = (j,i)\n",
    "    face1_tmp = face_arr_u[1][:,:,1:]\n",
    "    ## then add the first row from face 2 to the end of face 1\n",
    "    face1_mod = np.concatenate((face1_tmp, face_arr_u[2][:,:,:1]), axis=2)\n",
    "    \n",
    "    ## add modified face 1 by rotating and multiplying by -1\n",
    "    new_arr[:,:,0:185] = np.transpose(face1_mod[:,-185:,::-1],axes=(0,2,1))*-1\n",
    "    \n",
    "    ## add part of face 4 (Alaska)\n",
    "    new_arr[:,:,185+1080:] = face_arr_v[4][:,:,:535]\n",
    "\n",
    "    ## rotate by 90 deg twice to have Alaska on bottom left\n",
    "    new_arr = np.rot90(new_arr,2,axes=(1,2))\n",
    "    new_arr = new_arr *-1\n",
    "        \n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3233b80b-2c98-4dba-a5a3-36f0566d0f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_from_iter_num(iter_num):\n",
    "    \"\"\"\n",
    "    takes the model iteration that was pulled from the data's filename and converts it to its equivalent datetime\n",
    "    \"\"\"\n",
    "    ## Start time of the model is 5790000 (22.0319 years after 1992-01-01)\n",
    "    ## there are 120 seconds for each iteration and 86400 seconds per day\n",
    "    ## take the iteration number, convert to seconds, and calculate number of days since start of model\n",
    "    \n",
    "    \n",
    "    ## from Mike: \"Near the end of the simulation, I ran into some sort of instability so I changed the time step from 120 seconds to 60 seconds.\n",
    "    ## Usually I would change it back to 120 second after getting past the instability but I was kinda close to the end so I just let it ride with 60 seconds.\"\n",
    "    if iter_num > 1e7:\n",
    "        iter_num = iter_num/2\n",
    "    \n",
    "    num_days_since_start = iter_num*120 / 86400 ## divide iter_number by 86400 which is equal to the number of seconds in a day\n",
    "    \n",
    "    model_start_time = datetime(1992,1,1) # data.cal start time is 1992-01-01\n",
    "    timestamp = np.array([model_start_time + timedelta(days=num_days_since_start)], dtype='datetime64[ns]')\n",
    "    \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5661f20c-874d-4daf-97d0-3b9311d63514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_tar_gz_files(data_dir):\n",
    "    ## see if tar.gz files were already decompressed\n",
    "    data_files = list(data_dir.glob('*.data'))\n",
    "    if len(data_files)>0:\n",
    "        print(\"tar.gz files already unpacked\")\n",
    "    ## if not, open them\n",
    "    else:\n",
    "        ## pull list of all tar.gz files in directory\n",
    "        tar_gz_files = list(data_dir.glob('*.tar.gz'))\n",
    "        \n",
    "        ## unzip targz file\n",
    "        for file_path in tar_gz_files:\n",
    "            tar = tarfile.open(file_path, \"r:gz\")\n",
    "            tar.extractall(data_dir) # save files to same directory\n",
    "            tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "173d0927-b350-48fe-9897-f6f14d5a5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_2D_HHv2_ds(field_HH, model_grid_ds, timestamp, grid_point, da_name):\n",
    "    \n",
    "    ## get time bounds and center time\n",
    "    time_bnds, center_time = ecco.make_time_bounds_from_ds64(timestamp[0], 'AVG_DAY')\n",
    "    time_bnds_da = xr.DataArray(time_bnds.reshape(1,2), dims=['time', 'nv'])\n",
    "    \n",
    "    ## create DataArray for c point data\n",
    "    if grid_point == 'c':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','j','i'],\\\n",
    "                                coords={'time':(('time'), np.array([center_time]))})\n",
    "    \n",
    "    ## create DataArray for u point data\n",
    "    elif grid_point == 'u':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','j','i_g'],\\\n",
    "                                coords={'time':(('time'), np.array([center_time]))})\n",
    "    \n",
    "    ## create DataArray for v point data\n",
    "    elif grid_point == 'v':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','j_g','i'],\\\n",
    "                                coords={'time':(('time'), np.array([center_time]))})\n",
    "    \n",
    "    ## name the array\n",
    "    tmp_da.name = da_name\n",
    "    \n",
    "    ## add additional coordinates to dataset\n",
    "    tmp_ds = tmp_da.to_dataset().assign_coords({\n",
    "        'time_bnds':time_bnds_da,\\\n",
    "        'XC':model_grid_ds.XC,\\\n",
    "        'YC':model_grid_ds.YC,\\\n",
    "        'XG':model_grid_ds.XG,\\\n",
    "        'YG':model_grid_ds.YG,\\\n",
    "        'XC_bnds':model_grid_ds.XC_bnds,\\\n",
    "        'YC_bnds':model_grid_ds.YC_bnds,\\\n",
    "        'Zp1':model_grid_ds.Zp1})\n",
    "    \n",
    "    return tmp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a42f6a92-5dec-43f4-b2c8-7daa6a94a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3D_HHv2_ds(field_HH, model_grid_ds, timestamp, grid_point, da_name, k_face='center'):\n",
    "    \n",
    "    ## get time bounds and center time\n",
    "    time_bnds, center_time = ecco.make_time_bounds_from_ds64(timestamp[0], 'AVG_DAY')\n",
    "    time_bnds_da = xr.DataArray(time_bnds.reshape(1,2), dims=['time', 'nv'])\n",
    "    \n",
    "    if k_face == 'center':\n",
    "        \n",
    "        ## create DataArray for c point data, center\n",
    "        if grid_point == 'c':\n",
    "            tmp_da = xr.DataArray([field_HH], dims=['time', 'k','j','i'],\\\n",
    "                                    coords={'time':(('time'),np.array([center_time]))})\n",
    "            tmp_da['k'].attrs['axis']  = 'Z'\n",
    "            \n",
    "        ## create DataArray for u point data, center\n",
    "        elif grid_point == 'u':\n",
    "            tmp_da = xr.DataArray([field_HH], dims=['time', 'k','j','i_g'],\\\n",
    "                                    coords={'time':(('time'),np.array([center_time]))})\n",
    "            tmp_da['k'].attrs['axis']  = 'Z'\n",
    "            \n",
    "        ## create DataArray for v point data, center\n",
    "        elif grid_point == 'v':\n",
    "            tmp_da = xr.DataArray([field_HH], dims=['time', 'k','j_g','i'],\\\n",
    "                                    coords={'time':(('time'),np.array([center_time]))})\n",
    "            tmp_da['k'].attrs['axis']  = 'Z'            \n",
    "            \n",
    "    elif k_face == 'top':\n",
    "        ## create DataArray for c point data, top\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','k_l','j','i'],\\\n",
    "                                coords={'time':(('time'),np.array([center_time]))})\n",
    "        tmp_da['k_l'].attrs['axis']  = 'Z'\n",
    "\n",
    "    ## name the array\n",
    "    tmp_da.name = da_name\n",
    "        \n",
    "    ## add additional coordinates to dataset\n",
    "    tmp_ds = tmp_da.to_dataset().assign_coords({\n",
    "        'time_bnds':time_bnds_da,\\\n",
    "        'XC':model_grid_ds.XC,\\\n",
    "        'YC':model_grid_ds.YC,\\\n",
    "        'XG':model_grid_ds.XG,\\\n",
    "        'YG':model_grid_ds.YG,\\\n",
    "        'XC_bnds':model_grid_ds.XC_bnds,\\\n",
    "        'YC_bnds':model_grid_ds.YC_bnds,\\\n",
    "        'Z':model_grid_ds.Z,\\\n",
    "        'Zu':model_grid_ds.Zu,\\\n",
    "        'Zl':model_grid_ds.Zl,\\\n",
    "        'Zp1':model_grid_ds.Zp1})\n",
    "        \n",
    " #   tmp_da = add_geo_metadata(tmp_da)\n",
    "\n",
    "    return tmp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9e076f8-2547-4989-9ba8-e78da9bbdcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_2D_variable(data_dir, filename, var_tmp_table, vars_table, sassie_n1_geometry_ds):\n",
    "    \n",
    "    var_name = var_tmp_table['variable'].values[0]\n",
    "    n_skip = var_tmp_table['field_index'].values[0] * 1\n",
    "    grid_point = var_tmp_table.cgrid_point.values\n",
    "    \n",
    "    ## process binary data to compact format\n",
    "    data_compact, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=1, skip=n_skip)\n",
    "    \n",
    "    ## convert compact format to 5 faces\n",
    "    data_faces = sassie_n1_compact_to_faces_2D(data_compact)\n",
    "    \n",
    "    ## convert faces to HHv2 Arctic rectangle\n",
    "    ## data on u and v points need to be handled differently from c points\n",
    "    if var_tmp_table.data_type.values == 'V': # if it is a vector field\n",
    "        var_mate = var_tmp_table.mate.values[0]\n",
    "        var_table_mate = vars_table[vars_table.variable.values == var_mate]\n",
    "        var_mate_field_index = var_table_mate.field_index.values[0]\n",
    "        \n",
    "        if grid_point == 'v':\n",
    "            ## get u field\n",
    "            n_skip_u = var_mate_field_index\n",
    "            data_compact_u, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=1, skip=n_skip_u)\n",
    "            face_arr_u = sassie_n1_compact_to_faces_2D(data_compact_u)\n",
    "            \n",
    "            ## process v field\n",
    "            var_HHv2 = combine_sassie_N1_faces_to_HHv2_2D_v_point(data_faces, face_arr_u)\n",
    "            \n",
    "        elif grid_point == 'u':\n",
    "            ## get v field\n",
    "            n_skip_v = var_mate_field_index\n",
    "            data_compact_v, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=1, skip=n_skip_v)\n",
    "            face_arr_v = sassie_n1_compact_to_faces_2D(data_compact_v)\n",
    "            \n",
    "            ## process u field\n",
    "            var_HHv2 = combine_sassie_N1_faces_to_HHv2_2D_u_point(data_faces, face_arr_v)\n",
    "            \n",
    "    elif var_tmp_table.data_type.values == 'S': # if it is a scalar field\n",
    "        var_HHv2 = combine_sassie_N1_faces_to_HHv2_2D(data_faces)\n",
    "    \n",
    "    ## add timestamp adn create dataset\n",
    "    timestamp = timestamp_from_iter_num(iter_num)\n",
    "    var_HHv2_ds = make_2D_HHv2_ds(var_HHv2, sassie_n1_geometry_ds, timestamp, grid_point=grid_point, da_name=var_name)\n",
    "    \n",
    "    return var_HHv2_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48e32f93-f0fe-46f7-af05-b3af6da8e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_3D_variable(data_dir, filename, var_tmp_table, vars_table, sassie_n1_geometry_ds):\n",
    "    \n",
    "    var_name = var_tmp_table['variable'].values[0]\n",
    "    \n",
    "    ## there are 90 vertical levels; use index from table to identify how many fields to skip\n",
    "    n_skip = var_tmp_table['field_index'].values[0] * 90\n",
    "    var_k_face = var_tmp_table['k_face'].values[0]\n",
    "    grid_point = var_tmp_table.cgrid_point.values\n",
    "    \n",
    "    ## process binary data to compact format\n",
    "    data_compact, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=90, skip=n_skip)\n",
    "    \n",
    "    ## convert compact format to 5 faces\n",
    "    data_faces = sassie_n1_compact_to_faces_3D(data_compact)\n",
    "    \n",
    "    ## convert faces to HHv2 Arctic rectangle\n",
    "    ## data on u and v points need to be handled differently from c points\n",
    "    if var_tmp_table.data_type.values == 'V': # if it is a vector field\n",
    "        var_mate = var_tmp_table.mate.values[0]\n",
    "        var_table_mate = vars_table[vars_table.variable.values == var_mate]\n",
    "        var_mate_field_index = var_table_mate.field_index.values[0]\n",
    "        \n",
    "        if grid_point == 'v':\n",
    "            ## get u field\n",
    "            n_skip_u = var_mate_field_index * 90\n",
    "            data_compact_u, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=90, skip=n_skip_u)\n",
    "            face_arr_u = sassie_n1_compact_to_faces_3D(data_compact_u)\n",
    "            \n",
    "            ## process v field\n",
    "            var_HHv2 = combine_sassie_N1_faces_to_HHv2_3D_v_point(data_faces, face_arr_u)\n",
    "            \n",
    "        elif grid_point == 'u':\n",
    "            ## get v field\n",
    "            n_skip_v = var_mate_field_index * 90\n",
    "            data_compact_v, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=90, skip=n_skip_v)\n",
    "            face_arr_v = sassie_n1_compact_to_faces_3D(data_compact_v)\n",
    "            \n",
    "            ## process u field\n",
    "            var_HHv2 = combine_sassie_N1_faces_to_HHv2_3D_u_point(data_faces, face_arr_v)\n",
    "        \n",
    "    elif var_tmp_table.data_type.values == 'S': # if it is a scalar field\n",
    "        var_HHv2 = combine_sassie_N1_faces_to_HHv2_3D(data_faces) # c point\n",
    "    \n",
    "    ## add timestamp and create dataset\n",
    "    timestamp = timestamp_from_iter_num(iter_num)\n",
    "    var_HHv2_ds = make_3D_HHv2_ds(var_HHv2, sassie_n1_geometry_ds, timestamp, grid_point=grid_point, da_name=var_name, k_face=var_k_face)\n",
    "    \n",
    "    return var_HHv2_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "098f533a-f2e4-417f-85f7-33c3031d5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_dry_grid_cells(ds, var, geometry_ds, grid_point):\n",
    "    ## make copy of dataset\n",
    "    ds_tmp = ds.copy(deep=True)\n",
    "    \n",
    "    ## tracer points use maskC, u points use maskW, and v points use maskS\n",
    "    if grid_point == 'c':\n",
    "        ds_tmp[var] = ds[var].where(geometry_ds.maskC==True)\n",
    "        \n",
    "    elif grid_point == 'v':\n",
    "        ds_tmp[var] = ds[var].where(geometry_ds.maskS==True)\n",
    "        \n",
    "    elif grid_point == 'u':\n",
    "        ds_tmp[var] = ds[var].where(geometry_ds.maskW==True)\n",
    "    \n",
    "    return ds_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4a7e8aa-cd52-476c-be78-d90bfbee96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoding(ecco_ds, output_array_precision = np.float32):\n",
    "    \n",
    "    # Create NetCDF encoding directives\n",
    "    # ---------------------------------------------\n",
    "    # print('\\n... creating variable encodings')\n",
    "    # ... data variable encoding directives\n",
    "    \n",
    "    # Define fill values for NaN\n",
    "    if output_array_precision == np.float32:\n",
    "        netcdf_fill_value = nc4.default_fillvals['f4']\n",
    "\n",
    "    elif output_array_precision == np.float64:\n",
    "        netcdf_fill_value = nc4.default_fillvals['f8']\n",
    "    \n",
    "    dv_encoding = dict()\n",
    "    for dv in ecco_ds.data_vars:\n",
    "        dv_encoding[dv] =  {'zlib':True, \\\n",
    "                            'complevel':5,\\\n",
    "                            'shuffle':True,\\\n",
    "                            '_FillValue':netcdf_fill_value}\n",
    "\n",
    "    # ... coordinate encoding directives\n",
    "    # print('\\n... creating coordinate encodings')\n",
    "    coord_encoding = dict()\n",
    "    for coord in ecco_ds.coords:\n",
    "        # set default no fill value for coordinate\n",
    "        if output_array_precision == np.float32:\n",
    "            coord_encoding[coord] = {'_FillValue':None, 'dtype':'float32'}\n",
    "        elif output_array_precision == np.float64:\n",
    "            coord_encoding[coord] = {'_FillValue':None, 'dtype':'float64'}\n",
    "\n",
    "        # force 64 bit ints to be 32 bit ints\n",
    "        if (ecco_ds[coord].values.dtype == np.int32) or \\\n",
    "           (ecco_ds[coord].values.dtype == np.int64) :\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "        # fix encoding of time\n",
    "        if coord == 'time' or coord == 'time_bnds':\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "            if 'units' in ecco_ds[coord].attrs:\n",
    "                # apply units as encoding for time\n",
    "                coord_encoding[coord]['units'] = ecco_ds[coord].attrs['units']\n",
    "                # delete from the attributes list\n",
    "                del ecco_ds[coord].attrs['units']\n",
    "\n",
    "        elif coord == 'time_step':\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "    # ... combined data variable and coordinate encoding directives\n",
    "    encoding = {**dv_encoding, **coord_encoding}\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbb29622-6bea-4345-8b30-1549abaf163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_metadata(ds, var, var_filename_netcdf):   \n",
    "    title = 'SASSIE Ocean Model ' + var + ' Parameter for the Lat-Lon-Cap 1080 (llc1080) Native Model Grid (Version 1 Release 1)'\n",
    "    \n",
    "    ## edit specific metadata for these datasets\n",
    "    ds.attrs['author'] = 'Mike Wood, Marie Zahn, and Ian Fenty'\n",
    "    ds.attrs['comment'] = 'SASSIE llc1080 V1R1 fields are consolidated onto a single curvilinear grid face focusing on the Arctic domain using fields from the 5 faces of the lat-lon-cap 1080 (llc1080) native grid used in the original simulation.'\n",
    "    ds.attrs['id'] = '10.5067/XXXXX-XXXXX' # will update with DOI when avail\n",
    "    ds.attrs['geospatial_vertical_min'] = np.round(ds.Zu.min().values,1)\n",
    "    ds.attrs['geospatial_lat_min'] = np.round(ds.YC.min().values,1)\n",
    "    ds.attrs['metadata_link'] = 'https://cmr.earthdata.nasa.gov/search/collections.umm_json?ShortName=XXXX_L4_GEOMETRY_LLC1080GRID_V1R1' # will update with DOI when avail\n",
    "    ds.attrs['product_name'] = var_filename_netcdf\n",
    "    ds.attrs['time_coverage_end'] = str(ds.time_bnds.values[0][0])[:-10]\n",
    "    ds.attrs['time_coverage_start'] = str(ds.time_bnds.values[0][1])[:-10]\n",
    "    ds.attrs['product_version'] = 'Version 1, Release 1'\n",
    "    ds.attrs['program'] = 'NASA Physical Oceanography'\n",
    "    ds.attrs['source'] = 'The SASSIE ocean model simulation was produced by downscaling the global ECCO state estimate from 1/3 to 1/12 degree grid cells. The ECCO global solution provided initial and boundary conditions and atmospheric forcing.'\n",
    "    ds.attrs['references'] = 'TBD'\n",
    "    ds.attrs['summary'] = 'This dataset provides data variable and geometric parameters for the lat-lon-cap 1080 (llc1080) native model grid from the SASSIE ECCO ocean model Version 1 Release 1 (V1r1) ocean and sea-ice state estimate.'\n",
    "    ds.attrs['title'] = title\n",
    "    ds.attrs['uuid'] = str(uuid.uuid1())\n",
    "    \n",
    "    ## remove some attributes we don't need\n",
    "    attributes_to_remove = ['product_time_coverage_start', 'product_time_coverage_end',\\\n",
    "                            'geospatial_lat_resolution', 'geospatial_lon_resolution']\n",
    "    \n",
    "    ## add current time and date\n",
    "    current_time = datetime.now().isoformat()[0:19]\n",
    "    ds.attrs['date_created'] = current_time\n",
    "    ds.attrs['date_modified'] = current_time\n",
    "    ds.attrs['date_metadata_modified'] = current_time\n",
    "    ds.attrs['date_issued'] = current_time\n",
    "    \n",
    "    for attr in attributes_to_remove:\n",
    "        ds.attrs.pop(attr, None)\n",
    "        \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4559c954-1f03-47ba-998f-1ce043f3a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_dims(xr_dataset):\n",
    "    ## specify order of dims\n",
    "    tmp = xr_dataset[[\"time\",\"j\",\"i\",\"k\",\"j_g\",\"i_g\",\"k_u\",\"k_l\",\"k_p1\",\"nv\",\"nb\"]]\n",
    "    tmp = tmp.drop_indexes([\"nv\",\"nb\"]).reset_coords([\"nv\",\"nb\"], drop=True)\n",
    "    \n",
    "    ## reassign dataset to new dims\n",
    "    xr_ds_ordered = tmp.assign(xr_dataset)\n",
    "    \n",
    "    return xr_ds_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02038080-c67b-4451-bf7e-95121995b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sassie_netcdf_to_s3(var_HHv2_ds, output_dir, root_filename, var_filename_netcdf, var_name):\n",
    "    ## save netCDF files\n",
    "    \n",
    "    ## create encoding\n",
    "    encoding_var = create_encoding(var_HHv2_ds, output_array_precision = np.float32)\n",
    "    \n",
    "    ## stage netcdf on tmp directory on ec2\n",
    "    tmp_netcdf_dir = \"/home/jpluser/sassie/tmp_netcdf\"\n",
    "    var_HHv2_ds.to_netcdf(tmp_netcdf_dir / var_filename_netcdf, encoding = encoding_var)\n",
    "    var_HHv2_ds.close()\n",
    "    \n",
    "    ## push file to s3 cloud\n",
    "    mybucket = \"ecco-processed-data/SASSIE/N1/V1/HH/NETCDF/\" + var_name + \"_AVG_DAILY\"\n",
    "    cmd=f\"aws s3 cp {tmp_netcdf_dir} s3://{mybucket}/ --recursive --include '*.nc'\"\n",
    "    \n",
    "    ## remove tmp file\n",
    "    os.system(f\"rm -rf {tmp_netcdf_dir}/*\")\n",
    "    \n",
    "    print('\\n==== saved netcdf: ' + var_filename_netcdf + ' ====\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f50ac024-7115-479a-97fe-abf92d8bf656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sassie_HHv2_3D(face_arr, depth_level=0, vmin=None, vmax=None,\\\n",
    "    cmap='jet', axs = None, \\\n",
    "    show_colorbar=True):\n",
    "\n",
    "    tmp = combine_sassie_N1_faces_to_HHv2_3D(face_arr)\n",
    "\n",
    "    if vmin == None:\n",
    "        vmin = np.min(tmp)\n",
    "    if vmax == None:\n",
    "        vmax = np.max(tmp)\n",
    "\n",
    "    if axs == None:\n",
    "        plt.imshow(tmp[depth_level,:,:], origin='lower', interpolation='none',vmin=vmin,vmax=vmax, cmap=cmap)\n",
    "        if show_colorbar:\n",
    "            plt.colorbar()\n",
    "\n",
    "    else:\n",
    "        im1 = axs.imshow(tmp[depth_level,:,:], origin='lower', interpolation='none',vmin=vmin,vmax=vmax, cmap=cmap)\n",
    "        fig = plt.gcf()\n",
    "        if show_colorbar:\n",
    "            fig.colorbar(im1, ax=axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d68ec0bc-86c0-4f3d-8487-9319c130362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_HH_netcdfs(var, data_dir_ec2, metadata_dict, sassie_n1_geometry_ds, vars_table, root_dest_s3_name):\n",
    "    \n",
    "    ## loop through each variable that was requested --------------------------------------------\n",
    "    print('#### ==== processing:', var, '==== #### \\n')\n",
    "    \n",
    "    ## get root directory for variable and then define directory\n",
    "    var_tmp_table = vars_table[vars_table.variable.isin([var])]\n",
    "    root_filename = var_tmp_table.root_filename.values[0]\n",
    "    \n",
    "    ## loop through files in root directory\n",
    "    data_files = np.sort(list(data_dir_ec2.glob('*.data')))\n",
    "    \n",
    "    for file in data_files:\n",
    "        print('loading file: ', file)\n",
    "        \n",
    "        ## get filename\n",
    "        filename = str(file).split('/')[-1]\n",
    "        \n",
    "        ## 3D data processing\n",
    "        if var_tmp_table['n_dims'].values == '3D':\n",
    "            ## process dataset\n",
    "            var_HHv2_ds = process_3D_variable(data_dir_ec2, filename, var_tmp_table,\\\n",
    "                                              vars_table, sassie_n1_geometry_ds)\n",
    "        ## 2D data processing \n",
    "        elif var_tmp_table['n_dims'].values == '2D':\n",
    "            ## process dataset\n",
    "            var_HHv2_ds = process_2D_variable(data_dir_ec2, filename, var_tmp_table,\\\n",
    "                                              vars_table, sassie_n1_geometry_ds)\n",
    "        \n",
    "        ## mask land cells\n",
    "        var_HHv2_ds = mask_dry_grid_cells(var_HHv2_ds, var, sassie_n1_geometry_ds, grid_point=var_tmp_table['cgrid_point'].values)\n",
    "        \n",
    "        ## add metadata\n",
    "        global_latlon_metadata = metadata_dict['ECCOv4r4_global_metadata_for_all_datasets'] + metadata_dict['ECCOv4r4_global_metadata_for_latlon_datasets']\n",
    "        var_HHv2_ds = ecco.add_global_metadata(global_latlon_metadata, var_HHv2_ds, var_tmp_table['n_dims'].values[0])\n",
    "        var_HHv2_ds = ecco.add_coordinate_metadata(metadata['ECCOv4r4_coordinate_metadata_for_latlon_datasets'], var_HHv2_ds, less_output=True)\n",
    "        var_HHv2_ds, grouping_keywords = ecco.add_variable_metadata(metadata['ECCOv4r4_geometry_metadata_for_latlon_datasets'], var_HHv2_ds, less_output=True)\n",
    "        var_HHv2_ds, grouping_keywords = ecco.add_variable_metadata(metadata['ECCOv4r4_variable_metadata'], var_HHv2_ds, less_output=True)\n",
    "        \n",
    "        ## generate filename\n",
    "        center_time = var_HHv2_ds.time.values\n",
    "        yyyy_mm_dd = str(center_time)[2:6] + \"-\" + str(center_time)[7:9] + \"-\" + str(center_time)[10:12]\n",
    "        var_filename_netcdf = var + \"_day_mean_\" + yyyy_mm_dd + \"_ECCO_SASSIE_V1_HH_llc1080.nc\"\n",
    "        \n",
    "        ## tweak some of the global attributes\n",
    "        var_HHv2_ds = modify_metadata(var_HHv2_ds, var, var_filename_netcdf)\n",
    "        \n",
    "        ## reorder dims\n",
    "        var_HHv2_ds_ordered = reorder_dims(var_HHv2_ds)\n",
    "        \n",
    "        ## save netcdf\n",
    "        save_sassie_netcdf_to_s3(var_HHv2_ds_ordered, root_dest_s3_name, root_filename, var_filename_netcdf, var)\n",
    "        \n",
    "    # return(var_HHv2_ds_final)\n",
    "    print(\"######## processing complete ########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3a4f3-2a40-4289-a409-80f44b424b39",
   "metadata": {},
   "source": [
    "# Create routine to process files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ec772-bfb8-4282-8729-e6fa86acd2d7",
   "metadata": {},
   "source": [
    "Specify root directory and process all variables in that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4dd2a823-4aac-42f8-9145-c520c9e2cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_filenames = 'ocean_state_3D_day_mean'\n",
    "root_s3_name = 's3://ecco-model-granules/SASSIE/N1/'\n",
    "# files_to_process = -1 # -1 = all files; one number = that file index; two numbers = range from a to b\n",
    "files_to_process = [0,2]\n",
    "root_dest_s3_bucket = 's3://ecco-processed-data/SASSIE/N1/V1/HH/NETCDF/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf666217-d7b1-456b-abbb-ac77b4a91015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sassie_ecco_netcdfs(root_filenames, root_s3_name, root_dest_s3_name, files_to_process):\n",
    "    \n",
    "    ## --------------------------------------------\n",
    "    ## open model geometry from ec2\n",
    "    sassie_n1_geometry_ds = xr.open_dataset('/home/jpluser/sassie/GRID_GEOMETRY_SASSIE_HH_V1R1_NATIVE_LLC1080.nc')\n",
    "    \n",
    "    ## open table that includes metadata for all variables\n",
    "    vars_table = pd.read_csv('/home/jpluser/git_repos/SASSIE_downscale_ecco_v5/sassie_variables_table.csv', index_col=False)\n",
    "    \n",
    "    ## --------------------------------------------\n",
    "    ## load metadata \n",
    "    metadata_json_dir = '/home/jpluser/git_repos/ECCO-ACCESS/metadata/ECCOv4r4_metadata_json/'\n",
    "    \n",
    "    metadata_fields = ['ECCOv4r4_global_metadata_for_all_datasets',\n",
    "                       'ECCOv4r4_global_metadata_for_latlon_datasets',\n",
    "                       'ECCOv4r4_global_metadata_for_native_datasets',\n",
    "                       'ECCOv4r4_coordinate_metadata_for_1D_datasets',\n",
    "                       'ECCOv4r4_coordinate_metadata_for_latlon_datasets',\n",
    "                       'ECCOv4r4_coordinate_metadata_for_native_datasets',\n",
    "                       'ECCOv4r4_geometry_metadata_for_latlon_datasets',\n",
    "                       'ECCOv4r4_geometry_metadata_for_native_datasets',\n",
    "                       'ECCOv4r4_groupings_for_1D_datasets',\n",
    "                       'ECCOv4r4_groupings_for_latlon_datasets',\n",
    "                       'ECCOv4r4_groupings_for_native_datasets',\n",
    "                       'ECCOv4r4_variable_metadata',\n",
    "                       'ECCOv4r4_variable_metadata_for_latlon_datasets',\n",
    "                       'ECCOv4r4_dataset_summary']\n",
    "    \n",
    "    ## load metadata\n",
    "    metadata_dict = dict()\n",
    "    \n",
    "    for mf in metadata_fields:\n",
    "        mf_e = mf + '.json'\n",
    "        # print(mf_e)\n",
    "        with open(Path(metadata_json_dir + mf_e), 'r') as fp:\n",
    "            metadata_dict[mf] = json.load(fp)\n",
    "    \n",
    "    ## --------------------------------------------\n",
    "    ## loop through gz files in root directory and process all variables included in the dataset\n",
    "    \n",
    "    ## get list of gz files in s3 directory\n",
    "    s3 = []\n",
    "    s3 = s3fs.S3FileSystem(anon=False)\n",
    "    \n",
    "    # find filenames\n",
    "    file_list = np.sort(s3.glob(f'{root_s3_name}{root_filenames}/*tar.gz'))\n",
    "\n",
    "    # construct url form of filenames\n",
    "    data_urls = [\n",
    "            's3://' + f\n",
    "            for f in file_list\n",
    "        ]\n",
    "    \n",
    "    ## specify start and end indices or process all files   \n",
    "    if len(files_to_process) == 2: # two numbers indicates a range (two indices)\n",
    "        data_urls_select = data_urls[files_to_process[0]:files_to_process[1]]\n",
    "    elif len(files_to_process) == 1 and files_to_process[0] == -1: # process all files\n",
    "        data_urls_select = data_urls\n",
    "    elif len(files_to_process) == 1 and files_to_process[0] == 0: # process one file using number as index\n",
    "        data_urls_select = data_urls[files_to_process[0]]\n",
    "    else:\n",
    "        print(\"`files_to_process` needs to be -1, a single integer, or a list with two indicies specified\")\n",
    "    \n",
    "    for data_url in data_urls_select:\n",
    "        ## download tar.gz file from s3 cloud to ec2 tmp_dir\n",
    "        s3 = []\n",
    "        s3 = s3fs.S3FileSystem(anon=False)\n",
    "        s3.download(data_url, \"/home/jpluser/sassie/tmp_gz/\" + data_url.split(\"/\")[-1])\n",
    "        \n",
    "        ## decompress tar.gz file into *.data and *.meta files\n",
    "        data_dir_ec2 = Path('/home/jpluser/sassie/tmp_gz/')\n",
    "        unpack_tar_gz_files(data_dir_ec2)\n",
    "         \n",
    "        ## use table to identify which variables are in the dataset\n",
    "        vars_in_dataset = vars_table[vars_table.root_filename.isin([root_filenames])].variable.values\n",
    "    \n",
    "        ## loop through variables in dataset and generate netcdfs\n",
    "        for var in vars_in_dataset:\n",
    "        \n",
    "            ## generate netcdfs for variable\n",
    "            create_HH_netcdfs(var, data_dir_ec2, metadata_dict, sassie_n1_geometry_ds, vars_table, root_dest_s3_name)\n",
    "   \n",
    "        ## after processing is complete, delete data files on ec2\n",
    "        print(\"==== deleting ec2 data files ====\\n\")\n",
    "        \n",
    "        ## remove tmp tar.gz files\n",
    "        os.system(f\"rm -rf {str(data_dir_ec2)}/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d155aa2a-c798-4dce-a2f8-19c4c18c6080",
   "metadata": {},
   "source": [
    "Use python script and run an example:\n",
    "\n",
    "`python generate_sassie_ecco_netcdfs.py --root_filenames ocean_state_3D_day_mean --root_s3_name s3://ecco-model-granules/SASSIE/N1/ --root_dest_s3_name s3://ecco-processed-data/SASSIE/N1/V1/HH/NETCDF/ --files_to_process 10 12`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851bb86-ace1-4999-b767-b2642a524e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
