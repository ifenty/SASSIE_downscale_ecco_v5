{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e27f266-b8f4-4b04-84db-74102e7eea65",
   "metadata": {},
   "source": [
    "# Process SASSIE ocean model granules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "5c84f351-0d07-465d-9a4c-7d3910f915b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import netCDF4 as nc4\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "## import ECCO utils\n",
    "import sys\n",
    "sys.path.append('/Users/mzahn/github_others/ECCOv4-py')\n",
    "import ecco_v4_py as ecco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de92919a-28e4-4b43-8b7e-0fa6ef481866",
   "metadata": {},
   "source": [
    "ECCO github docs: https://github.com/ECCO-GROUP/ECCOv4-py/blob/master/ecco_v4_py/netcdf_product_generation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4755fb0a-e017-4367-aaa4-72dcf180321a",
   "metadata": {},
   "source": [
    "*Info from Ian/Mike:*\n",
    "\n",
    "TIME<br>\n",
    "data.cal start time is 1992-01-01<br>\n",
    "Start time of the model is 5790000 (22.0319 years after 1992-01-01)<br>\n",
    "Which is 2014-01-06T16:00:00<br>\n",
    "Model simulation goes to around end of 2021<br>\n",
    "\n",
    "FILES<br>\n",
    "Each gz file has 14 files, each for one day<br>\n",
    "199 (files)*14 (days per gz file) = 2,786 (days)/365 (days per year)= 7.6329 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00fe74-ad78-4651-b07c-49b1858359b5",
   "metadata": {},
   "source": [
    "From *.meta file:\n",
    " nDims = [   2 ];\n",
    " dimList = [\n",
    "         40,         1,        40,\n",
    "     102600,         1,    102600\n",
    " ];\n",
    " dataprec = [ 'float32' ];\n",
    " nrecords = [          3 ];\n",
    " timeStepNumber = [    5810400 ];\n",
    " timeInterval = [  6.971616000000E+08  6.972480000000E+08 ];\n",
    " missingValue = [ -9.99000000000000E+02 ];\n",
    " nFlds = [    3 ];\n",
    " fldList = {\n",
    " 'SIarea  ' 'SIheff  ' 'SIhsnow '\n",
    " };\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393d80c-b86d-4429-86b2-e2488e98c5fa",
   "metadata": {},
   "source": [
    "## ECCO routines \n",
    "\n",
    "### routines to convert between SASSIE N1 faces and compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "dfaeed1d-3bd1-4e7e-93f5-bd0836682a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sassie_N1_field(file_dir, fname, nk=1, skip=0):\n",
    "    num_cols = 680*4 + 1080\n",
    "    num_rows = 1080\n",
    "    \n",
    "    time_level = int(fname.split('.data')[0].split('.')[-1])\n",
    "    \n",
    "    tmp_compact = ecco.load_binary_array(file_dir, fname, \\\n",
    "                                    num_rows, num_cols, nk=nk, skip=skip, filetype='>f4')\n",
    "\n",
    "    return tmp_compact, time_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e3fd3a-ddfa-4d1a-8932-2c9a78c43f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sassie_n1_compact_to_faces_2D(sassie_n1_compact):\n",
    "    sassie_faces = dict()\n",
    "    n = 680\n",
    "    \n",
    "    # Face 1 \n",
    "    start_row = 0\n",
    "    end_row = n\n",
    "    sassie_faces[1] = sassie_n1_compact[start_row:end_row,:]\n",
    "\n",
    "    # Face 2\n",
    "    start_row = end_row\n",
    "    end_row = start_row + n\n",
    "\n",
    "    sassie_faces[2] = sassie_n1_compact[start_row:end_row,:]\n",
    "    \n",
    "    # Face 3\n",
    "    start_row = end_row\n",
    "    end_row = start_row + 1080\n",
    "    sassie_faces[3] = sassie_n1_compact[start_row:end_row:,:]\n",
    "    \n",
    "    #Face 4\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[4] = sassie_n1_compact[start_row:end_row].reshape(1080, n)\n",
    "\n",
    "    #Face 5\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[5] = sassie_n1_compact[start_row:end_row].reshape(1080, n)\n",
    "\n",
    "    return sassie_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09477f84-8112-4716-b74c-42c741039eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sassie_n1_compact_to_faces_3D(sassie_n1_compact):\n",
    "    sassie_faces = dict()\n",
    "    n = 680\n",
    "    \n",
    "    # Face 1 \n",
    "    start_row = 0\n",
    "    end_row = n\n",
    "    sassie_faces[1] = sassie_n1_compact[:,start_row:end_row,:]\n",
    "\n",
    "    # Face 2\n",
    "    start_row = end_row\n",
    "    end_row = start_row + n\n",
    "    sassie_faces[2] = sassie_n1_compact[:,start_row:end_row,:]\n",
    "    \n",
    "    # Face 3\n",
    "    start_row = end_row\n",
    "    end_row = start_row + 1080\n",
    "    sassie_faces[3] = sassie_n1_compact[:,start_row:end_row:,:]\n",
    "    \n",
    "    #Face 4\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[4] = sassie_n1_compact[:,start_row:end_row].reshape(90, 1080, n)\n",
    "\n",
    "    #Face 5\n",
    "    start_row = end_row\n",
    "    end_row = end_row + 680\n",
    "    sassie_faces[5] = sassie_n1_compact[:,start_row:end_row].reshape(90, 1080, n)\n",
    "\n",
    "    return sassie_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75aa6e3f-1790-4db9-b0d2-0f58b63f82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to slice small pieces from Faces 1 and 4 and combine them to the Arctic Face 3\n",
    "def combine_sassie_N1_faces_to_HHv2_2D(face_arr):\n",
    "    # dimensions of the final Arctic HH field. 535+185+1080=1800\n",
    "    new_arr = np.zeros((1080, 1800)) \n",
    "    # cut out sections we want and assign them to location on HH\n",
    "    new_arr[:, 185:185 + 1080] = face_arr[3]\n",
    "    # rotate Face 1 to line up with orientation of Face 3\n",
    "    new_arr[:, 0:185] = np.flipud(face_arr[1][-185:,:].T) # flip and transpose\n",
    "    new_arr[:, 185 + 1080:] = face_arr[4][:,:535]\n",
    "\n",
    "    new_arr = np.rot90(new_arr,2) # rotate it 180 so Greenland/AK are on bottom\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "194e7405-099a-42ea-9b21-57a5a38d0bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to slice small pieces from Faces 1 and 4 and combine them to the Arctic Face 3\n",
    "def combine_sassie_N1_faces_to_HHv2_3D(face_arr):\n",
    "    # dimensions of the final Arctic HH field. 535+185+1080=1800 ; 90 vertical levels\n",
    "    new_arr = np.zeros((90, 1080, 1800)) \n",
    "    # cut out sections we want and assign them to location on HH\n",
    "    new_arr[:, :, 185:185 + 1080] = face_arr[3]\n",
    "    # rotate Face 1 to line up with orientation of Face 3\n",
    "    new_arr[:, :, 0:185] = np.transpose(face_arr[1][:,-185:,::-1],axes=(0,2,1)) # flip and transpose\n",
    "    new_arr[:, :, 185 + 1080:] = face_arr[4][:,:,:535]\n",
    "\n",
    "    new_arr = np.rot90(new_arr,2,axes=(1,2)) # rotate it 180 so Greenland/AK are on bottom\n",
    "    return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f50ac024-7115-479a-97fe-abf92d8bf656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sassie_HHv2_3D(face_arr, depth_level=0, vmin=None, vmax=None,\\\n",
    "    cmap='jet', axs = None, \\\n",
    "    show_colorbar=True):\n",
    "\n",
    "    tmp = combine_sassie_N1_faces_to_HHv2_3D(face_arr)\n",
    "\n",
    "    if vmin == None:\n",
    "        vmin = np.min(tmp)\n",
    "    if vmax == None:\n",
    "        vmax = np.max(tmp)\n",
    "\n",
    "    if axs == None:\n",
    "        plt.imshow(tmp[depth_level,:,:], origin='lower', interpolation='none',vmin=vmin,vmax=vmax, cmap=cmap)\n",
    "        if show_colorbar:\n",
    "            plt.colorbar()\n",
    "\n",
    "    else:\n",
    "        im1 = axs.imshow(tmp[depth_level,:,:], origin='lower', interpolation='none',vmin=vmin,vmax=vmax, cmap=cmap)\n",
    "        fig = plt.gcf()\n",
    "        if show_colorbar:\n",
    "            fig.colorbar(im1, ax=axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "173d0927-b350-48fe-9897-f6f14d5a5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_2D_HHv2_da(field_HH, model_grid_ds, da_name):\n",
    "    tmp_da = xr.DataArray(field_HH, dims=['j','i'],\\\n",
    "                            coords={'XC': (('j','i'), model_grid_ds.XC.values),\\\n",
    "                                    'YC': (('j','i'), model_grid_ds.YC.values)})\n",
    "    \n",
    "    tmp_da.name = da_name\n",
    "    # tmp_da = add_geo_metadata(tmp_da)\n",
    "    \n",
    "    return tmp_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "32a57a14-f6a4-4b7f-8c97-7a5cff293b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_HHv2_ds = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        CS=([\"j\", \"i\"], angleCS_faces_HHv2),\n",
    "        SN=([\"j\", \"i\"], angleSN_faces_HHv2),\n",
    "        rAc=([\"j\",\"i\"], rAc_HHv2),\n",
    "        dxG=([\"j_g\",\"i\"], dxG_HHv2),\n",
    "        dyG=([\"j\",\"i_g\"], dyG_HHv2),\n",
    "    ),\n",
    "    coords=dict(\n",
    "        i   =([\"i\"], i_array),\n",
    "        i_g =([\"i_g\"], i_g_array),\n",
    "        j   =([\"j\"], j_array),\n",
    "        j_g =([\"j_g\"], j_g_array),\n",
    "        k   =([\"k\"], k_array),\n",
    "        k_u =([\"k_u\"], k_u_array),\n",
    "    ),\n",
    "    attrs=dict(description=\"HHv2 model geometry data.\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "a42f6a92-5dec-43f4-b2c8-7daa6a94a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3D_HHv2_da(field_HH, model_grid_ds, timestamp, da_name='NAME', k_face='center'):\n",
    "    if k_face == 'center':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time', 'k','j','i'],\\\n",
    "                                coords={'XC': (('j','i'), model_grid_ds.XC.values),\\\n",
    "                                        'YC': (('j','i'), model_grid_ds.YC.values),\\\n",
    "                                        'Z': (('k'), model_grid_ds.Z.values),\\\n",
    "                                        'Zu':(('k'),model_grid_ds.Zu.values),\\\n",
    "                                        'Zl':(('k'),model_grid_ds.Zl.values),\\\n",
    "                                        'time':(('time'),timestamp)})\n",
    "        tmp_da['k'].attrs['axis']  = 'Z'\n",
    "\n",
    "    elif k_face == 'top':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','k_l','j','i'],\\\n",
    "                                coords={'XC': (('j','i'), model_grid_ds.XC.values),\\\n",
    "                                        'YC': (('j','i'), model_grid_ds.YC.values),\\\n",
    "                                        'Z': (('k'), model_grid_ds.Z.values),\\\n",
    "                                        'Zu':(('k'),model_grid_ds.Zu.values),\\\n",
    "                                        'Zl':(('k'),model_grid_ds.Zl.values),\\\n",
    "                                        'time':(('time'),timestamp)})\n",
    "        tmp_da['k_l'].attrs['axis']  = 'Z'\n",
    "\n",
    "    elif k_face == 'bottom':\n",
    "        tmp_da = xr.DataArray([field_HH], dims=['time','k_u','j','i','time'],\\\n",
    "                                coords={'XC': (('j','i'), model_grid_ds.XC.values),\\\n",
    "                                        'YC': (('j','i'), model_grid_ds.YC.values),\\\n",
    "                                        'Z': (('k'), model_grid_ds.Z.values),\\\n",
    "                                        'Zu':(('k'),model_grid_ds.Zu.values),\\\n",
    "                                        'Zl':(('k'),model_grid_ds.Zl.values),\\\n",
    "                                        'time':(('time'),timestamp)})\n",
    "        tmp_da['k_u'].attrs['axis']  = 'Z'\n",
    "\n",
    "\n",
    "    tmp_da['Z'].attrs['long_name'] = 'grid cell depth at center'\n",
    "    tmp_da['Zu'].attrs['long_name'] = 'grid cell depth at bottom'\n",
    "    tmp_da['Zl'].attrs['long_name'] = 'grid cell depth at top'\n",
    "\n",
    "    tmp_da['Z'].attrs['units'] = 'm'\n",
    "    tmp_da['Zl'].attrs['units'] = 'm'\n",
    "    tmp_da['Zu'].attrs['units'] = 'm'\n",
    "\n",
    "    tmp_da.name = da_name\n",
    " #   tmp_da = add_geo_metadata(tmp_da)\n",
    "\n",
    "    return tmp_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "3233b80b-2c98-4dba-a5a3-36f0566d0f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_from_iter_num(iter_num):\n",
    "    \"\"\"\n",
    "    takes the model iteration that was pulled from the data's filename and converts it to its equivalent datetime\n",
    "    \"\"\"\n",
    "    ## Start time of the model is 5790000 (22.0319 years after 1992-01-01)\n",
    "    ## there are 120 seconds for each iteration and 86400 seconds per day\n",
    "    ## take the iteration number, convert to seconds, and calculate number of days since start of model\n",
    "    \n",
    "    num_days_since_start = iter_num*120 / 86400 ## divide iter_number by 86400 which is equal to the number of seconds in a day\n",
    "    model_start_time = datetime(1992,1,1) # data.cal start time is 1992-01-01\n",
    "    \n",
    "    timestamp = np.array([model_start_time + timedelta(days=num_days_since_start)], dtype='datetime64[ns]')\n",
    "    \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "f4a7e8aa-cd52-476c-be78-d90bfbee96e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoding(ecco_ds, output_array_precision = np.float32):\n",
    "    \n",
    "    # Create NetCDF encoding directives\n",
    "    # ---------------------------------------------\n",
    "    print('\\n... creating variable encodings')\n",
    "    # ... data variable encoding directives\n",
    "    \n",
    "    # Define fill values for NaN\n",
    "    if output_array_precision == np.float32:\n",
    "        netcdf_fill_value = nc4.default_fillvals['f4']\n",
    "\n",
    "    elif output_array_precision == np.float64:\n",
    "        netcdf_fill_value = nc4.default_fillvals['f8']\n",
    "    \n",
    "    dv_encoding = dict()\n",
    "    for dv in ecco_ds.data_vars:\n",
    "        dv_encoding[dv] =  {'zlib':True, \\\n",
    "                            'complevel':5,\\\n",
    "                            'shuffle':True,\\\n",
    "                            '_FillValue':netcdf_fill_value}\n",
    "\n",
    "    # ... coordinate encoding directives\n",
    "    print('\\n... creating coordinate encodings')\n",
    "    coord_encoding = dict()\n",
    "    for coord in ecco_ds.coords:\n",
    "        # set default no fill value for coordinate\n",
    "        if output_array_precision == np.float32:\n",
    "            coord_encoding[coord] = {'_FillValue':None, 'dtype':'float32'}\n",
    "        elif output_array_precision == np.float64:\n",
    "            coord_encoding[coord] = {'_FillValue':None, 'dtype':'float64'}\n",
    "\n",
    "        # force 64 bit ints to be 32 bit ints\n",
    "        if (ecco_ds[coord].values.dtype == np.int32) or \\\n",
    "           (ecco_ds[coord].values.dtype == np.int64) :\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "        # fix encoding of time\n",
    "        if coord == 'time' or coord == 'time_bnds':\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "            if 'units' in ecco_ds[coord].attrs:\n",
    "                # apply units as encoding for time\n",
    "                coord_encoding[coord]['units'] = ecco_ds[coord].attrs['units']\n",
    "                # delete from the attributes list\n",
    "                del ecco_ds[coord].attrs['units']\n",
    "\n",
    "        elif coord == 'time_step':\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "    # ... combined data variable and coordinate encoding directives\n",
    "    encoding = {**dv_encoding, **coord_encoding}\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3a4f3-2a40-4289-a409-80f44b424b39",
   "metadata": {},
   "source": [
    "# Create routine to process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f788e0d-3738-4238-ac42-9943c7c108da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sassie_n1_geometry_ds = xr.open_dataset('/Users/mzahn/data/SASSIE/GRID_GEOMETRY_SASSIE_HH_V1r1_native_llc1080.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35cedabe-60f3-4cfb-8067-b5e949a994be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## open example file \n",
    "# targz_file = tarfile.open('./data/tar_gz_files/seaice_state_day_mean.0005810000.tar.gz') \n",
    "targz_file = tarfile.open('/Users/mzahn/data/SASSIE/SASSIE_examples/ocean_state_2D_day_mean/ocean_state_2D_day_mean.0005800000.tar.gz')\n",
    "\n",
    "## extracting file to produce *.data and *.meta files\n",
    "targz_file.extractall('/Users/mzahn/data/SASSIE/SASSIE_examples/ocean_state_2D_day_mean/data') \n",
    "targz_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "b136be08-b8d2-4263-b1cb-70e9b90f69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## model output directories\n",
    "sassie_llc1080_data_dir = '/Users/mzahn/data/SASSIE/SASSIE_examples/'\n",
    "\n",
    "## ocean 3D data directory\n",
    "ocean_3d_file_dir = 'ocean_state_3D_day_mean/'\n",
    "ocean_3d_file_subdir = 'ocean_state_3D_day_mean.0005790000/' # includes 14 data files from compressed file\n",
    "\n",
    "## where to save netCDFs with directories for each variable\n",
    "mds_output_dir = '/Users/mzahn/data/SASSIE/SASSIE_netcdfs/'\n",
    "\n",
    "## identify data directories\n",
    "data_dir = Path(sassie_llc1080_data_dir + ocean_3d_file_dir + ocean_3d_file_subdir) # using ocean 3d as first example\n",
    "data_files = np.sort(list(data_dir.glob(mds_file + '*data')))\n",
    "\n",
    "## identify number of fields in dataset\n",
    "meta_files = np.sort(list(data_dir.glob(mds_file + '*meta')))\n",
    "nFlds = [int(i) for i in meta_file[11].split('=')[-1].split() if i.isdigit()][0] # extract number from string\n",
    "fldList = meta_file[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "58949c08-e6d6-4f9a-99e5-2efe9e76a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading file:  /Users/mzahn/data/SASSIE/SASSIE_examples/ocean_state_3D_day_mean/ocean_state_3D_day_mean.0005790000/ocean_state_3D_day_mean.0005796720.data\n",
      "load_binary_array: loading file /Users/mzahn/data/SASSIE/SASSIE_examples/ocean_state_3D_day_mean/ocean_state_3D_day_mean.0005790000/ocean_state_3D_day_mean.0005796720.data\n",
      "load_binary_array: data array shape  (180, 3800, 1080)\n",
      "load_binary_array: data array type  >f4\n",
      "\n",
      "... creating variable encodings\n",
      "\n",
      "... creating coordinate encodings\n",
      "\n",
      "... creating variable encodings\n",
      "\n",
      "... creating coordinate encodings\n",
      "loading file:  /Users/mzahn/data/SASSIE/SASSIE_examples/ocean_state_3D_day_mean/ocean_state_3D_day_mean.0005790000/ocean_state_3D_day_mean.0005797440.data\n",
      "load_binary_array: loading file /Users/mzahn/data/SASSIE/SASSIE_examples/ocean_state_3D_day_mean/ocean_state_3D_day_mean.0005790000/ocean_state_3D_day_mean.0005797440.data\n",
      "load_binary_array: data array shape  (180, 3800, 1080)\n",
      "load_binary_array: data array type  >f4\n",
      "\n",
      "... creating variable encodings\n",
      "\n",
      "... creating coordinate encodings\n",
      "\n",
      "... creating variable encodings\n",
      "\n",
      "... creating coordinate encodings\n"
     ]
    }
   ],
   "source": [
    "## loop through each time step ------\n",
    "\n",
    "## loop through files\n",
    "for file in data_files[0:2]:\n",
    "    print('loading file: ', file)\n",
    "    \n",
    "    filename = str(file).split('/')[-1]\n",
    "    \n",
    "    if nFlds == 2:\n",
    "        ## process binary data to compact format\n",
    "        # nk=180 because there are two variables each with 90 vertical levels\n",
    "        var1_compact, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=90, skip=0)\n",
    "        var2_compact, iter_num = load_sassie_N1_field(str(data_dir), filename, nk=90, skip=90)\n",
    "        \n",
    "        ## convert compact format to 5 faces\n",
    "        var1_faces = sassie_n1_compact_to_faces_3D(data_compact[0:90,:,:]) # process first variable\n",
    "        var2_faces = sassie_n1_compact_to_faces_3D(data_compact[90:180,:,:]) # process first variable\n",
    "        \n",
    "        ## convert faces to HHv2 Arctic rectangle\n",
    "        var1_HHv2 = combine_sassie_N1_faces_to_HHv2_3D(var1_faces)\n",
    "        var2_HHv2 = combine_sassie_N1_faces_to_HHv2_3D(var2_faces)\n",
    "        \n",
    "        ## Create DataArrays from HHv2\n",
    "        var1_name = meta_file[13].split()[0].strip(\"' \\t\")\n",
    "        var2_name = meta_file[13].split()[2].strip(\"' \\t\")\n",
    "        \n",
    "        ## add timestamp to dataset\n",
    "        timestamp = timestamp_from_iter_num(iter_num)\n",
    "        \n",
    "        var1_HHv2_da = make_3D_HHv2_da(var1_HHv2, sassie_n1_geometry_ds, timestamp, da_name=var1_name, k_face='center')\n",
    "        var2_HHv2_da = make_3D_HHv2_da(var2_HHv2, sassie_n1_geometry_ds, timestamp, da_name=var2_name, k_face='center')\n",
    "        \n",
    "        ## convert DataArrays to datasets\n",
    "        var1_HHv2_ds = var1_HHv2_da.to_dataset()\n",
    "        var2_HHv2_ds = var2_HHv2_da.to_dataset()\n",
    "\n",
    "        # save netCDF files\n",
    "        mds_output_dir_var1 = Path(mds_output_dir + ocean_3d_file_dir + var1_name)\n",
    "        mds_output_dir_var2 = Path(mds_output_dir + ocean_3d_file_dir + var2_name)\n",
    "        \n",
    "        mds_output_dir_var1.mkdir(parents=True, exist_ok=True) # create output directory if it doesn't already exist\n",
    "        mds_output_dir_var2.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        encoding_var1 = create_encoding(var1_HHv2_ds, output_array_precision = np.float32)\n",
    "        encoding_var2 = create_encoding(var2_HHv2_ds, output_array_precision = np.float32)\n",
    "        \n",
    "        var1_filename_netcdf = \"SASSIE_HH_\" + var1_name + \"_\" + str(time_level) + \".nc\"\n",
    "        var2_filename_netcdf = \"SASSIE_HH_\" + var2_name + \"_\" + str(time_level) + \".nc\"\n",
    "        \n",
    "        var1_HHv2_ds.to_netcdf(mds_output_dir_var1 / var1_filename_netcdf, encoding = encoding_var1)\n",
    "        var2_HHv2_ds.to_netcdf(mds_output_dir_var2 / var2_filename_netcdf, encoding = encoding_var2)\n",
    "        \n",
    "        var1_HHv2_ds.close()\n",
    "        var2_HHv2_ds.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
