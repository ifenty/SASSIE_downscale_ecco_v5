{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38bdc3b6-75ad-42e9-bf60-2700e71048ec",
   "metadata": {},
   "source": [
    "## Extract surface layer from 3D field and save new 2D field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5905a394-b0e0-4797-83b0-049b6aa6aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c323f-2fbf-486b-bb8f-a2b06f9cd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to manually add key\n",
    "sassie_key =\n",
    "sassie_secret ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0c4137-9e4f-42ec-aaed-967f1d637608",
   "metadata": {},
   "outputs": [],
   "source": [
    "## s3 directories\n",
    "sassie_s3_netcdf_dir = 's3://podaac-dev-sassie/ECCO_model/N1/V1/HH/NETCDF/'\n",
    "sassie_s3_netcdf_dir_surface = 's3://podaac-dev-sassie/ECCO_model/N1/V1/HH/NETCDF_3D_SURF/' # directory for surface 3D fields\n",
    "\n",
    "## variable to process\n",
    "var_3d = 'THETA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33db9f8-c521-4cc7-9a21-f5855dc166bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoding(ecco_ds, output_array_precision = np.float32):\n",
    "    \n",
    "    # Create NetCDF encoding directives\n",
    "    # ---------------------------------------------\n",
    "    # print('\\n... creating variable encodings')\n",
    "    # ... data variable encoding directives\n",
    "    \n",
    "    # Define fill values for NaN\n",
    "    if output_array_precision == np.float32:\n",
    "        netcdf_fill_value = nc4.default_fillvals['f4']\n",
    "\n",
    "    elif output_array_precision == np.float64:\n",
    "        netcdf_fill_value = nc4.default_fillvals['f8']\n",
    "    \n",
    "    dv_encoding = dict()\n",
    "    for dv in ecco_ds.data_vars:\n",
    "        dv_encoding[dv] =  {'compression':'zlib',\\\n",
    "                            'complevel':5,\\\n",
    "                            'shuffle':False,\\\n",
    "                            'fletcher32': False,\\\n",
    "                            '_FillValue':netcdf_fill_value}\n",
    "\n",
    "    # ... coordinate encoding directives\n",
    "    coord_encoding = dict()\n",
    "    \n",
    "    for coord in ecco_ds.coords:\n",
    "        # set default no fill value for coordinate\n",
    "        if output_array_precision == np.float32:\n",
    "            coord_encoding[coord] = {'_FillValue':None, 'dtype':'float32'}\n",
    "        elif output_array_precision == np.float64:\n",
    "            coord_encoding[coord] = {'_FillValue':None, 'dtype':'float64'}\n",
    "\n",
    "        # force 64 bit ints to be 32 bit ints\n",
    "        if (ecco_ds[coord].values.dtype == np.int32) or \\\n",
    "           (ecco_ds[coord].values.dtype == np.int64) :\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "        # fix encoding of time\n",
    "        if coord == 'time' or coord == 'time_bnds':\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "            if 'units' in ecco_ds[coord].attrs:\n",
    "                # apply units as encoding for time\n",
    "                coord_encoding[coord]['units'] = ecco_ds[coord].attrs['units']\n",
    "                # delete from the attributes list\n",
    "                del ecco_ds[coord].attrs['units']\n",
    "\n",
    "        elif coord == 'time_step':\n",
    "            coord_encoding[coord]['dtype'] ='int32'\n",
    "\n",
    "    # ... combined data variable and coordinate encoding directives\n",
    "    encoding = {**dv_encoding, **coord_encoding}\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33408907-b187-424b-a190-4ffa01d60ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_surface_layer_from_3d(var_3d, sassie_s3_netcdf_dir, ec2_scratch_dir, sassie_key, sassie_secret):\n",
    "\n",
    "    ## initialize s3 system\n",
    "    s3 = s3fs.S3FileSystem(anon=False, key=sassie_key, secret=sassie_secret) \n",
    "    \n",
    "    ## list all files\n",
    "    nc_file_list = np.sort(s3.glob(f'{sassie_s3_netcdf_dir}{var_3d}_AVG_DAILY/*.nc'))\n",
    "\n",
    "    print(f'\\n> Looking for files on {sassie_s3_netcdf_dir}{var_3d}_AVG_DAILY/')\n",
    "    print(f'... num files  : {len(nc_file_list)}')\n",
    "    print(f'... first file : {nc_file_list[0]}')\n",
    "    print(f'... last file  : {nc_file_list[-1]}')\n",
    "    \n",
    "    ## append \"s3://\" to create url in order to open the dataset\n",
    "    print(f'\\n> Preparing list of files to process')\n",
    "    nc_file_list_urls = []\n",
    "    for file in nc_file_list:\n",
    "        file_url_tmp = f\"s3://{file}\"\n",
    "        nc_file_list_urls.append(file_url_tmp)\n",
    "    \n",
    "    ## loop through each file, extract surface, and save new netCDF\n",
    "    for file_url in nc_file_list_urls[0:1]:\n",
    "        \n",
    "        print(f\"\\n... opening {file_url}\")\n",
    "        s3_file = s3.open(file_url)\n",
    "        s3_file_ec2 = xr.open_dataset(s3_file)\n",
    "        s3_file_ec2.close()\n",
    "     \n",
    "        ## isolate the surface layer\n",
    "        print(f\"... extracting surface layer\\n\")\n",
    "        tmp_surface = s3_file_ec2.isel(k=[0], k_u=[0], k_l=[0], k_p1=slice(0,2))\n",
    "        \n",
    "        ## edit typo in metadata\n",
    "        tmp_surface.k_p1.attrs['comment'] = \"Top and bottom of model tracer cell.\"\n",
    "        \n",
    "        # print(s3_file_ec2)\n",
    "    \n",
    "        ## save newly generated 2D surface layer dataset to scratch directory\n",
    "        print(f\"... saving surface netCDF dataset to scratch directory {ec2_scratch_dir}\")\n",
    "    \n",
    "        ## edit filename to indicate that it is a surface layer file (not 3D)\n",
    "        filename_split = file.split(\"/\")[-1].split(\"day\")\n",
    "        netcdf_filename_new = f\"{filename_split[0]}SURFACE_day{filename_split[1]}\"\n",
    "\n",
    "        ## create encoding\n",
    "        encoding_var = create_encoding(tmp_surface, output_array_precision = np.float32)\n",
    "        \n",
    "        tmp_surface.to_netcdf(f\"{ec2_scratch_dir}/{netcdf_filename_new}.nc\", encoding = encoding_var)\n",
    "        print(f\"\\n* * * * saved netcdf to {ec2_scratch_dir}/{netcdf_filename_new}.nc* * * *\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e3c80d-66bc-4ab2-b32e-aa0394a7015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_nc_dir_from_ec2(ec2_scratch_dir, root_dest_s3_name, var_name):\n",
    "    \"\"\"\n",
    "    Pushes the netcdf files from a directory to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        ec2_scratch_dir (str): The path to the directory containing the netcdf files on the EC2 instance.\n",
    "        root_dest_s3_name (str): The root name of the S3 bucket where the files will be pushed.\n",
    "        var_name (str): The name of the variable used to create the S3 bucket.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    ## push file to s3 bucket\n",
    "    mybucket = root_dest_s3_name + var_name + \"_AVG_DAILY_SURF\"\n",
    "    nc_files = list(ec2_scratch_dir.glob('*.nc'))\n",
    "\n",
    "    print(f'\\n>pushing netcdf files in {ec2_scratch_dir} to s3 bucket : {mybucket}')\n",
    "    print(f'... looking for *.nc files in {ec2_scratch_dir}')\n",
    "    print(f'... found {len(nc_files)} nc files to upload')\n",
    "\n",
    "    if len(nc_files)>0:\n",
    "        cmd=f\"aws s3 cp {ec2_scratch_dir} {mybucket}/ --recursive --include '*.nc' --no-progress > /dev/null 2>&1\"\n",
    "        print(f'... aws command: {cmd}')\n",
    "        with suppress_stdout():\n",
    "           os.system(cmd)\n",
    "    else:\n",
    "        print(\"... nothing to upload!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11044b4d-a824-4919-8916-5fa9897d0f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_surface_layer(var_3d, sassie_s3_netcdf_dir, ec2_nvme_scratch_dir, sassie_key, sassie_secret, root_dest_s3_name):\n",
    "\n",
    "    ## process variable\n",
    "    print(f\"= = = = = processing {var_3d} = = = = =\\n\")\n",
    "\n",
    "    ## create temporary scratch directory on ec2\n",
    "    nc_root_dir_ec2 =  Path(f\"{ec2_nvme_scratch_dir}/tmp_nc/{var_3d}_AVG_DAILY_SURF\")\n",
    "    print(f'... temporary nc directory {nc_root_dir_ec2}/n')\n",
    "    nc_root_dir_ec2.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    ## extract surface layer from 3D netcdfs and save to scratch directory\n",
    "    save_surface_layer_from_3d(var_3d, sassie_s3_netcdf_dir, nc_root_dir_ec2, sassie_key, sassie_secret)\n",
    "\n",
    "    ## push all local netcdfs on ec2 to the cloud\n",
    "    push_nc_dir_from_ec2(nc_root_dir_ec2, root_dest_s3_name, var_3d)\n",
    "\n",
    "    ## clean up local scratch disk\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0bdfa15d-6199-40c3-8212-18b4d897abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_3d = [\n",
    "    \"SALT\",\n",
    "    \"THETA\",\n",
    "    \"UVEL\",\n",
    "    \"VVEL\",\n",
    "    \"WVEL\",\n",
    "    \"KPPdiffS\",\n",
    "    \"KPPdiffT\",\n",
    "    \"KPPviscA\",\n",
    "    \"PHIHYD\",\n",
    "    \"PHIHYDcR\",\n",
    "    \"RHOAnoma\",\n",
    "    \"ADVr_SLT\",\n",
    "    \"ADVr_TH\",\n",
    "    \"ADVx_SLT\",\n",
    "    \"ADVx_TH\",\n",
    "    \"ADVy_SLT\",\n",
    "    \"ADVy_TH\",\n",
    "    \"DFrE_SLT\",\n",
    "    \"DFrE_TH\",\n",
    "    \"DFrI_SLT\",\n",
    "    \"DFrI_TH\",\n",
    "    \"DFxE_SLT\",\n",
    "    \"DFxE_TH\",\n",
    "    \"DFyE_SLT\",\n",
    "    \"DFyE_TH\",\n",
    "    \"UVELMASS\",\n",
    "    \"VVELMASS\",\n",
    "    \"WVELMASS\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "01c97e0e-6bba-4da0-bb3d-8d7f77d7e376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fields_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d0502-7dcf-4bbe-8248-4526895950e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
